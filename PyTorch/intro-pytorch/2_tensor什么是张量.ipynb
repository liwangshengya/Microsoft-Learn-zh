{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensors\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices. \n",
        "In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
        "\n",
        "<img alt=\"Image showing ndnumpy and dimensional tensors\" src=\"images/2-tensor-1.png\" width=\"60%\"/>\n",
        "\n",
        "Tensors are similar to `NumPy’s`ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory address, eliminating the need to copy data will a capability called `bridge-to-np-label`. Tensors are also optimized for automatic differentiation (we'll see more about that later in the Autograd unit). If you’re familiar with `ndarrays`, you’ll be right at home with the Tensor API. If not, follow along!\n",
        "\n",
        "Let's start by setting up our environment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensors\n",
        "\n",
        "张量是一种专门的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用张量来编码一个模型的输入和输出，以及模型的参数。\n",
        "\n",
        "<img alt=\"Image showing ndnumpy and dimensional tensors\" src=\"images/2-tensor-1.png\" width=\"60%\"/>\n",
        "\n",
        "张量类似于`NumPy`的ndarrays，只是张量可以在GPU或其他硬件加速器上运行。事实上，张量和NumPy数组通常可以共享相同的底层内存地址，不需要复制数据，这种能力称为 `bridge-to-np-label`。张量也为自动微分进行了优化（我们将在后面的Autograd单元中看到更多的内容）。如果你熟悉 `ndarrays`，你就会对张量API很熟悉。如果不熟悉，那就跟着做吧!\n",
        "\n",
        "让我们从设置我们的环境开始。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initializing a Tensor\n",
        "\n",
        "Tensors can be initialized in various ways. Take a look at the following examples:\n",
        "\n",
        "## Directly from data\n",
        "\n",
        "Tensors can be created directly from data. The data type is automatically inferred.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 初始化一个张量\n",
        "\n",
        "张量可以通过不同的方式被初始化。请看下面的例子。\n",
        "\n",
        "## 直接来自数据\n",
        "\n",
        "张量可以直接从数据中创建。数据类型是自动推断出来的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From a NumPy array(来自Numpy数组)\n",
        "\n",
        "Tensors can be created from NumPy arrays and vice versa.  Since, numpy _'np_array'_ and tensor _'x_np'_ share the same memory location here, changing the value for one will change the other.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "张量可以从NumPy数组中创建，反之亦然。 由于numpy _'np_array'_和tensor _'x_np'_在这里共享同一个内存位置，改变一个的值就会改变另一个。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy np_array value: \n",
            " [[1 2]\n",
            " [3 4]] \n",
            "\n",
            "Tensor x_np value: \n",
            " tensor([[1, 2],\n",
            "        [3, 4]], dtype=torch.int32) \n",
            "\n",
            "Numpy np_array after * 2 operation: \n",
            " [[2 4]\n",
            " [6 8]] \n",
            "\n",
            "Tensor x_np value after modifying numpy array: \n",
            " tensor([[2, 4],\n",
            "        [6, 8]], dtype=torch.int32) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "\n",
        "print(f\"Numpy np_array value: \\n {np_array} \\n\")\n",
        "print(f\"Tensor x_np value: \\n {x_np} \\n\")\n",
        "\n",
        "np.multiply(np_array, 2, out=np_array)\n",
        "\n",
        "print(f\"Numpy np_array after * 2 operation: \\n {np_array} \\n\")\n",
        "print(f\"Tensor x_np value after modifying numpy array: \\n {x_np} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From another tensor:\n",
        "\n",
        "The new tensor retains the properties (shape, data type) of the argument tensor, unless explicitly overridden.\n",
        "\n",
        "新的张量保留了参数张量的属性（形状、数据类型），除非明确重写。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.2476, 0.2297],\n",
            "        [0.6623, 0.8990]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With random or constant values:\n",
        "\n",
        "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.  Shape shows the number of rows and columns in the tensor.  E.g. shape = (# of rows, # of columns).\n",
        "\n",
        "``shape``是张量维度的元组。在下面的函数中，它决定了输出张量的维度。 形状显示张量中的行和列的数量。 例如，shape = (# of rows, # of columns).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.4424, 0.4927, 0.5646],\n",
            "        [0.7742, 0.0868, 0.3927]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attributes of a Tensor\n",
        "\n",
        "Tensor attributes describe their shape, data type, and the device on which they are stored.\n",
        "\n",
        "张量属性描述了它们的形状、数据类型以及存储它们的设备。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Operations on Tensors\n",
        "\n",
        "There are more than 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing). For sampling and reviewing, you'll find a comprehensive description [here](https://pytorch.org/docs/stable/torch.html).\n",
        "\n",
        "Each of these operations can be run on the GPU (at typically higher speeds than on a\n",
        "CPU).\n",
        "- CPUs have up to 16 cores. Cores are units that do the actual computation. Each core processes tasks in a sequential order (one task at a time).\n",
        "- GPUs have 1000s of cores.  GPU cores handle computations in parallel processing. Tasks are divided and processed across the different cores. That's what makes GPUs faster than CPUs in most cases. GPUs perform better with large data than small data. GPU are typically used for high-intensive computation of graphics or neural networks (we'll learn more about that later in the Neural Network unit).\n",
        "- PyTorch can use the Nvidia CUDA library to take advantage of their GPU cards.\n",
        "\n",
        "<img alt=\"Diagram showing workload between cpu and gpu\" src=\"images/2-tensor-2.png\"/>\n",
        "\n",
        "By default, tensors are created on the CPU. Tensors can also be computed to GPUs; to do that, you need to move them using the `.to` method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!\n",
        "\n",
        "\n",
        "有 100 多种张量运算，包括算术、线性代数、矩阵运算（转置、索引、切片）。 对于抽样和审查，您会在[此处](https://pytorch.org/docs/stable/torch.html) 找到全面的描述。\n",
        "\n",
        "这些操作中的每一个都可以在 GPU 上运行（速度通常比在\n",
        "中央处理器）。\n",
        "- CPU 最多有 16 个内核。 核心是进行实际计算的单元。 每个核心按顺序处理任务（一次一个任务）。\n",
        "- GPU 有 1000 个核心。 GPU 核心处理并行处理中的计算。 任务在不同的核心上进行划分和处理。 这就是 GPU 在大多数情况下比 CPU 更快的原因。 GPU 处理大数据比处理小数据表现更好。 GPU 通常用于图形或神经网络的高强度计算（我们将在稍后的神经网络单元中了解更多相关信息）。\n",
        "- PyTorch 可以使用 Nvidia CUDA 库来利用他们的 GPU 卡。\n",
        "默认情况下，张量是在 CPU 上创建的。 张量也可以计算到 GPU； 为此，您需要使用 .to 方法移动它们（在检查 GPU 可用性之后）。 请记住，跨设备复制大型张量在时间和内存方面可能会很昂贵！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try out some of the operations from the list.\n",
        "If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.\n",
        "\n",
        "尝试列表中的一些操作。如果您熟悉 NumPy API，您会发现使用 Tensor API 轻而易举。\n",
        "\n",
        "## Standard numpy-like indexing and slicing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First row:  tensor([1., 1., 1., 1.])\n",
            "First column:  tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print('First row: ',tensor[0])\n",
        "print('First column: ', tensor[:, 0])\n",
        "print('Last column:', tensor[..., -1])\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joining tensors\n",
        "You can use `torch.cat` to concatenate a sequence of tensors along a given dimension.\n",
        "`torch.stack` is another tensor joining option that is subtly different from ``torch.cat``.\n",
        "\n",
        "您可以使用 `torch.cat` 沿给定维度连接一系列张量。\n",
        "`torch.stack` 是另一个张量连接选项，与 ``torch.cat`` 略有不同。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8340,  1.2816, -0.8016, -0.1380],\n",
            "        [ 0.4937,  1.6309, -0.3095,  0.0281],\n",
            "        [-0.3200, -0.7727,  1.9499,  0.4538],\n",
            "        [-0.2160, -0.5823, -0.3773, -0.6511],\n",
            "        [ 0.2444, -0.2264, -0.4019, -0.4739]])\n",
            "tensor([[[-1.2215,  1.4380, -1.2521,  0.1376],\n",
            "         [ 1.0475,  1.1167,  1.6312,  0.7649],\n",
            "         [-1.1945, -0.3838,  1.1138, -1.5418]],\n",
            "\n",
            "        [[-0.0770, -0.3508,  0.4797, -3.2875],\n",
            "         [-0.7917,  1.3613,  0.4882,  0.3139],\n",
            "         [ 0.7273,  0.7288,  0.4425, -1.0175]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.2215,  1.4380, -1.2521,  0.1376],\n",
              "         [-0.0770, -0.3508,  0.4797, -3.2875]],\n",
              "\n",
              "        [[ 1.0475,  1.1167,  1.6312,  0.7649],\n",
              "         [-0.7917,  1.3613,  0.4882,  0.3139]],\n",
              "\n",
              "        [[-1.1945, -0.3838,  1.1138, -1.5418],\n",
              "         [ 0.7273,  0.7288,  0.4425, -1.0175]]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "a=torch.randn(3,4) #随机生成一个shape（3，4）的tensort\n",
        "b=torch.randn(2,4) #随机生成一个shape（2，4）的tensor\n",
        "\n",
        "print(torch.cat([a,b],dim=0) )\n",
        "#返回一个shape（5，4）的tensor\n",
        "#把a和b拼接成一个shape（5，4）的tensor，\n",
        "#可理解为沿着行增加的方向（即纵向）拼接\n",
        "\n",
        "a=torch.randn(3,4)\n",
        "b=torch.randn(3,4)\n",
        "\n",
        "c=torch.stack([a,b],dim=0)\n",
        "#返回一个shape(2,3,4)的tensor,新增的维度2分别指向a和b\n",
        "print(c)\n",
        "d=torch.stack([a,b],dim=1)\n",
        "#返回一个shape（3,2,4）的tensor，新增的维度2分别指向相应的a的第i行和b的第i行\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arithmetic operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "#这计算两个张量之间的矩阵乘法。 y1、y2、y3 将具有相同的值\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(tensor)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "#这计算逐元素乘积。 z1、z2、z3 将具有相同的值\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single-element tensors\n",
        "If you have a one-element tensor, for example by aggregating all\n",
        "values of a tensor into one value, you can convert it to a Python\n",
        "numerical value using `item()`:\n",
        "\n",
        "如果你有一个单元素张量，例如通过聚合所有\n",
        "张量的值转换为一个值，你可以将它转换为 Python\n",
        "使用 item() 的数值："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()  \n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-place operations\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix. \n",
        "For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
        "\n",
        "> **Note:** In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\n",
        "\n",
        "将结果存储到操作数中的操作称为就地操作。 它们由 ``_`` 后缀表示。\n",
        "例如：``x.copy_(y)``、``x.t_()``，将改变``x``。\n",
        "\n",
        "> **注意：**就地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。 因此，不鼓励使用它们。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ],
      "source": [
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bridge with NumPy\n",
        "\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory\n",
        "locations, and changing one will change\tthe other.\n",
        "\n",
        "CPU上的张量和NumPy数组可以共享它们的底层内存\n",
        "位置，改变一个就会改变另一个。\n",
        "### Tensor to NumPy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A change in the tensor reflects in the NumPy array.\n",
        "\n",
        "张量的变化反映在 NumPy 数组中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NumPy array to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changes in the NumPy array reflects in the tensor.\n",
        "\n",
        "NumPy 数组的变化反映在张量中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "py38_default"
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b1e50b9be59f7fbb2b520cd9d0f0de3401ffabf78f05d86508e8222ccb7b50f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
