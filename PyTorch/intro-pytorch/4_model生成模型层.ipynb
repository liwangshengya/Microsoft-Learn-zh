{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQvaC_8yzs8Z"
      },
      "source": [
        "# What is a neural network\n",
        "\n",
        "Neural network is a collection of neurons that are connected by layers. Each neuron is a small\n",
        "computing unit that performs simple calculations to collectively solve a problem. They are \n",
        "organized in layers. There are 3 types of layers: input layer, hidden layer and \n",
        "outter layer.  Each layer contains a number of neurons, except for the input layer. Neural networks mimic the way a human brain processes information.\n",
        "\n",
        "神经网络是一个由神经元组成的集合，这些神经元通过层层连接。每个神经元都是一个小型计算单元，执行简单的计算以共同解决一个问题。它们被组织成层。有3种类型的层：输入层、隐藏层和 外层。 每一层都包含一些神经元，除了输入层。神经网络模仿了人脑处理信息的方式。\n",
        "\n",
        "<img alt=\"Diagram showing different types of layers in a neural network\" src=\"images/4-model-1.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-XSKl4kzs8b"
      },
      "source": [
        "## Components of a neural network\n",
        "\n",
        "- **Activation function** determines whether a neuron should be activated or not. The computations that happen in a neural network include applying an activation function. If a neuron activates, then it means the input is important.  The are different kinds of activation functions. The choice of which activation function to use depends on what you want the output to be. Another important role of an activation function is to add non-linearity to the model.\n",
        "    - _Binary_ used to set an output node to 1 if function result is positive and 0 if the function result is negative. $f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ 1, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\n",
        "    - _Sigmod_ is used to predict the probability of an output node being between 0 and 1.  $f(x) = {\\large \\frac{1}{1+e^{-x}}} $\n",
        "    - _Tanh_ is used to predict if an output node is between 1 and -1.  Used in classification use cases.  $f(x) = {\\large \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}} $\n",
        "    - _ReLU_ used to set the output node to 0 if fuction result is negative and keeps the result value if the result is a positive value.  $f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ x, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\n",
        "- **Weights** influence how well the output of our network will come close to the expected output value. As an input enters the neuron, it gets multiplied by a weight value and the resulting output is either observed, or passed to the next layer in the neural network. Weights for all neurons in a layer are organized into one tensor\n",
        "- **Bias** makes up the difference between the activation function's output and its intended output. A low bias suggest that the network is making more assumptions about the form of the output, whereas a high bias value makes less assumptions about the form of the output. \n",
        "\n",
        "<img alt=\"Diagram showing neural computation\" src=\"images/4-model-2.png\" />\n",
        "\n",
        "We can say that an output $y$ of a neural network layer with weights $W$ and bias $b$ is computed as summation of the inputs multiply by the weights plus the bias $x = \\sum{(weights * inputs) + bias} $, where $f(x)$ is the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx96T5ik0Tm2"
      },
      "source": [
        "- **激活函数(Activation function )**决定了一个神经元是否应该被激活。发生在神经网络中的计算包括应用激活函数。如果一个神经元被激活，那么这意味着输入是重要的。 有不同种类的激活函数。选择使用哪种激活函数取决于你希望输出是什么。激活函数的另一个重要作用是为模型添加非线性。\n",
        "    - _Binary_用于在函数结果为正时将输出节点设置为1，在函数结果为负时设置为0。$f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ 1, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\n",
        "    - _Sigmod_用于预测输出节点在0和1之间的概率。$f(x) = {\\large \\frac{1}{1+e^{-x}}} $\n",
        "    - _Tanh_用于预测一个输出节点是否在1和-1之间。 在分类用例中使用。 $f(x) = {large\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}} $ $\n",
        "    - _ReLU_用于在运算结果为负值时将输出节点设置为0，如果结果为正值，则保留结果值。 $f(x)= {\\small \\begin{cases} 0, & \\text{if } x < 0\\\\ x, & \\text{if } x\\geq 0\\\\ \\end{cases}}$\n",
        "- **权重(Weights)**影响着我们网络的输出会有多大程度的接近预期输出值。当一个输入进入神经元时，它被乘以一个权重值，然后产生的输出被观察，或者被传递到神经网络的下一层。一层中所有神经元的权重被组织成一个张量\n",
        "- **偏置(Bias)**构成了激活函数的输出和其预期输出之间的差异。低偏置表明网络对输出的形式做了更多的假设，而高偏置值对输出的形式做了更少的假设。\n",
        "\n",
        "<img alt=\"显示神经计算的图表\" src=\"images/4-model-2.png\" />\n",
        "\n",
        "我们可以说，具有权重$W$和偏置$b$的神经网络层的输出$y$被计算为输入乘以权重加偏置的总和 $x = sum{(权重*输入)+偏置} $，其中$f(x)$是激活函数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCgQSjjtzs8c"
      },
      "source": [
        "# Build a neural network\n",
        "\n",
        "Neural networks are comprised of layers/modules that perform operations on data. \n",
        "The `torch.nn` namespace provides all the building blocks you need to \n",
        "build your own neural network. Every module in PyTorch subclasses the `nn.Module`. \n",
        "A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n",
        "building and managing complex architectures easily.\n",
        "\n",
        "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3wwFgOq5zs8d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSFzs0tZzs8d"
      },
      "source": [
        "## Get a hardware device for training\n",
        "\n",
        "We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let's check to see if \n",
        "`torch.cuda` is available, else we continue to use the CPU.\n",
        "如果可用，我们希望能够在 GPU 等硬件加速器上训练我们的模型。 让我们看看是否\n",
        "`torch.cuda` 可用，否则我们继续使用 CPU。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnjPu-fmzs8e",
        "outputId": "42187bd1-d2b8-4546-8fa6-4a370f7f60dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyINbmPRzs8e"
      },
      "source": [
        "## Define the class\n",
        "\n",
        "We define our neural network by subclassing `nn.Module`, and \n",
        "initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements\n",
        "the operations on input data in the `forward` method.\n",
        "\n",
        "Our neural network are composed of the following:\n",
        "- The input layer with 28x28 or 784 features/pixels.\n",
        "- The first linear module takes the input 784 features and transforms it to a hidden layer with 512 features\n",
        "- The ReLU activation function will be applied in the transformation\n",
        "- The second linear module take 512 features as input from the first hidden layer and transforms it to the next hidden layer with 512 features\n",
        "- The ReLU activation function will be applied in the transformation\n",
        "- The third linear module take 512 features as input from the second hidden layer and transforms it to the output layer with 10, which is the number of classes\n",
        "- The ReLU activation function will be applied in the transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb2BCJpv2QT3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYY9Nutq2JBo"
      },
      "source": [
        "我们通过子类化`nn.Module`来定义我们的神经网络，并在`__init__`中初始化神经网络层。\n",
        "在 `__init__`中初始化神经网络层。每个`nn.Module`子类都在`forward`方法中实现了\n",
        "在``forward``方法中对输入数据进行操作。\n",
        "\n",
        "我们的神经网络由以下部分组成。\n",
        "- 输入层有28x28或784个特征/像素。\n",
        "- 第一个线性模块接收输入的784个特征，并将其转换为具有512个特征的隐藏层。\n",
        "- ReLU激活函数将被应用于转换中。\n",
        "- 第二个线性模块将512个特征作为第一个隐藏层的输入，并将其转换到有512个特征的下一个隐藏层。\n",
        "- 在转换过程中会应用ReLU激活函数。\n",
        "- 第三个线性模块从第二隐藏层获取512个特征的输入，并将其转换到输出层，输出层有10个特征，这就是类的数量。\n",
        "- 在转换过程中，将应用ReLU激活函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Ge16Rofzs8f"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y9w8597zs8f"
      },
      "source": [
        "We create an instance of `NeuralNetwork`, and move it to the `device`, and print \n",
        "it's structure.\n",
        "\n",
        "我们创建一个 NeuralNetwork 的实例，并将其移动到 device，然后打印\n",
        "它的结构。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noywbuWezs8f",
        "outputId": "b9fffa18-1ff7-4a5d-d962-1a447f0277ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jze1raN8zs8f"
      },
      "source": [
        "To use the model, we pass it the input data. This executes the model's `forward`, along with some background operations. However, do not call `model.forward()` directly! Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class.\n",
        "\n",
        "We get the prediction densities by passing it through an instance of the `nn.Softmax`.\n",
        "\n",
        "\n",
        "为了使用这个模型，我们把输入数据传给它。这将执行模型的`forward`，以及一些后台操作。然而，不要直接调用`model.forward()`! 在输入数据上调用模型会返回一个10维的张量，其中包含每个类别的原始预测值。\n",
        "\n",
        "我们通过一个`nn.Softmax`的实例来获得预测密度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtpNtGHVzs8f",
        "outputId": "1b509c62-ed9c-43b6-a19c-8de746842e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X) \n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SdDEiiIzs8g"
      },
      "source": [
        "## Weight and Bias\n",
        "\n",
        "\n",
        "The `nn.Linear` module randomly initializes the ${weights}$  and ${ bias}$ for each layer and internally stores the values in Tensors.\n",
        "\n",
        "`nn.Linear` 模块随机初始化每一层的 ${weights}$ 和 ${bias}$ 并将值内部存储在 Tensors 中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BmpMvV0zs8g",
        "outputId": "e551b095-b845-4571-f0e5-e140974620af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Linear weights: Parameter containing:\n",
            "tensor([[ 0.0140,  0.0338,  0.0249,  ..., -0.0208, -0.0341,  0.0157],\n",
            "        [-0.0241, -0.0277, -0.0175,  ..., -0.0084,  0.0227, -0.0085],\n",
            "        [-0.0157,  0.0120, -0.0311,  ..., -0.0223, -0.0129,  0.0160],\n",
            "        ...,\n",
            "        [ 0.0351,  0.0063, -0.0206,  ..., -0.0268, -0.0244,  0.0356],\n",
            "        [ 0.0018, -0.0008, -0.0207,  ..., -0.0245, -0.0340,  0.0129],\n",
            "        [ 0.0204,  0.0179, -0.0313,  ..., -0.0181,  0.0178, -0.0175]],\n",
            "       device='cuda:0', requires_grad=True) \n",
            "\n",
            "First Linear weights: Parameter containing:\n",
            "tensor([ 0.0148, -0.0243, -0.0101, -0.0226, -0.0127, -0.0003,  0.0335,  0.0145,\n",
            "         0.0327,  0.0309, -0.0350, -0.0297,  0.0004, -0.0180, -0.0281,  0.0251,\n",
            "         0.0105,  0.0103, -0.0053, -0.0149,  0.0189,  0.0101,  0.0325,  0.0296,\n",
            "         0.0186,  0.0081, -0.0234, -0.0252, -0.0098, -0.0150,  0.0254, -0.0209,\n",
            "         0.0346, -0.0043, -0.0031, -0.0221, -0.0257, -0.0237,  0.0318, -0.0109,\n",
            "         0.0314, -0.0163,  0.0124, -0.0039,  0.0033, -0.0130,  0.0130,  0.0147,\n",
            "        -0.0074, -0.0084,  0.0036, -0.0235, -0.0151, -0.0107,  0.0059,  0.0176,\n",
            "        -0.0072, -0.0226, -0.0312, -0.0190,  0.0049,  0.0208,  0.0022,  0.0261,\n",
            "         0.0291, -0.0313,  0.0047, -0.0085, -0.0096, -0.0124,  0.0244, -0.0085,\n",
            "        -0.0223, -0.0279, -0.0344,  0.0070, -0.0234,  0.0135,  0.0121,  0.0201,\n",
            "        -0.0184, -0.0128, -0.0040, -0.0079, -0.0082,  0.0053, -0.0187,  0.0341,\n",
            "        -0.0013, -0.0250,  0.0068,  0.0086, -0.0106, -0.0009, -0.0295, -0.0308,\n",
            "        -0.0005,  0.0090, -0.0159,  0.0206, -0.0144, -0.0223,  0.0149, -0.0216,\n",
            "         0.0128,  0.0208,  0.0072,  0.0349, -0.0342,  0.0284,  0.0066,  0.0151,\n",
            "        -0.0170,  0.0139, -0.0228, -0.0164, -0.0301,  0.0259, -0.0103,  0.0273,\n",
            "         0.0209,  0.0128,  0.0329, -0.0257,  0.0248, -0.0270, -0.0307,  0.0062,\n",
            "        -0.0157,  0.0102,  0.0200,  0.0056, -0.0051,  0.0147, -0.0325, -0.0323,\n",
            "        -0.0042,  0.0204,  0.0199, -0.0135,  0.0210,  0.0164, -0.0225, -0.0073,\n",
            "        -0.0357, -0.0170, -0.0212,  0.0255,  0.0327,  0.0239, -0.0136,  0.0318,\n",
            "        -0.0155, -0.0195,  0.0259, -0.0271, -0.0210,  0.0185,  0.0182,  0.0127,\n",
            "         0.0287, -0.0140,  0.0148, -0.0307,  0.0353,  0.0306,  0.0029,  0.0099,\n",
            "         0.0177,  0.0056, -0.0261,  0.0104,  0.0014,  0.0107,  0.0351, -0.0146,\n",
            "         0.0142, -0.0353, -0.0258, -0.0301, -0.0232,  0.0165, -0.0106, -0.0187,\n",
            "        -0.0145, -0.0107,  0.0164, -0.0165,  0.0185,  0.0297, -0.0011, -0.0174,\n",
            "         0.0098, -0.0280, -0.0349, -0.0227,  0.0204,  0.0276,  0.0297,  0.0339,\n",
            "         0.0033,  0.0141, -0.0294, -0.0291,  0.0248,  0.0254,  0.0094, -0.0163,\n",
            "         0.0333,  0.0011, -0.0076, -0.0240,  0.0034,  0.0231,  0.0014,  0.0068,\n",
            "        -0.0325, -0.0004, -0.0017,  0.0240,  0.0113, -0.0067,  0.0232, -0.0126,\n",
            "         0.0144,  0.0215,  0.0229, -0.0306,  0.0336, -0.0315, -0.0314,  0.0250,\n",
            "        -0.0317, -0.0162, -0.0038,  0.0001, -0.0307, -0.0062, -0.0262,  0.0170,\n",
            "         0.0345,  0.0265,  0.0337, -0.0325,  0.0334, -0.0203, -0.0215,  0.0069,\n",
            "         0.0012, -0.0305,  0.0313, -0.0016, -0.0245, -0.0322, -0.0335,  0.0130,\n",
            "         0.0139,  0.0221,  0.0222, -0.0127,  0.0278, -0.0021,  0.0079, -0.0192,\n",
            "         0.0132, -0.0289,  0.0294,  0.0353,  0.0290,  0.0088,  0.0281, -0.0029,\n",
            "        -0.0228, -0.0087, -0.0302,  0.0318, -0.0331, -0.0310, -0.0264, -0.0346,\n",
            "        -0.0101,  0.0119, -0.0290,  0.0001, -0.0075,  0.0251,  0.0026,  0.0107,\n",
            "        -0.0176, -0.0079,  0.0136, -0.0249, -0.0050, -0.0145, -0.0337, -0.0138,\n",
            "        -0.0291,  0.0015,  0.0245,  0.0262, -0.0269,  0.0347, -0.0313,  0.0353,\n",
            "         0.0007,  0.0204, -0.0320,  0.0168, -0.0040,  0.0035, -0.0124, -0.0260,\n",
            "         0.0005, -0.0219,  0.0090,  0.0129, -0.0340, -0.0009,  0.0037,  0.0092,\n",
            "        -0.0238, -0.0263,  0.0155,  0.0192,  0.0094,  0.0236,  0.0136,  0.0150,\n",
            "        -0.0111,  0.0298,  0.0037, -0.0283, -0.0337, -0.0269,  0.0165, -0.0034,\n",
            "         0.0238,  0.0255, -0.0152, -0.0029,  0.0116,  0.0268, -0.0317,  0.0249,\n",
            "         0.0253,  0.0103, -0.0007, -0.0106,  0.0172, -0.0328, -0.0160,  0.0291,\n",
            "        -0.0077,  0.0106, -0.0133, -0.0164,  0.0164,  0.0272,  0.0001, -0.0267,\n",
            "        -0.0074,  0.0237, -0.0155, -0.0210,  0.0036, -0.0226,  0.0136, -0.0047,\n",
            "         0.0221,  0.0245,  0.0058,  0.0103,  0.0270,  0.0015,  0.0276,  0.0155,\n",
            "         0.0103, -0.0071, -0.0347, -0.0335,  0.0278, -0.0319,  0.0193, -0.0148,\n",
            "         0.0301, -0.0079, -0.0328,  0.0218, -0.0255,  0.0091,  0.0024, -0.0065,\n",
            "         0.0326,  0.0196,  0.0296,  0.0336, -0.0152,  0.0036, -0.0125, -0.0036,\n",
            "        -0.0096, -0.0298, -0.0246, -0.0083,  0.0303, -0.0056, -0.0164, -0.0078,\n",
            "        -0.0073, -0.0329, -0.0175,  0.0249,  0.0095,  0.0334,  0.0136,  0.0036,\n",
            "         0.0120,  0.0187, -0.0199, -0.0054,  0.0272,  0.0164, -0.0160, -0.0017,\n",
            "        -0.0136,  0.0357, -0.0059,  0.0209, -0.0289,  0.0248,  0.0255, -0.0355,\n",
            "         0.0039,  0.0151,  0.0182, -0.0160,  0.0314, -0.0239, -0.0202,  0.0326,\n",
            "        -0.0078,  0.0221, -0.0102,  0.0041, -0.0242, -0.0168,  0.0041, -0.0062,\n",
            "        -0.0273, -0.0198, -0.0146, -0.0044, -0.0098,  0.0238,  0.0256,  0.0305,\n",
            "         0.0104,  0.0061, -0.0159,  0.0236,  0.0186,  0.0246, -0.0140,  0.0317,\n",
            "         0.0044,  0.0268, -0.0173,  0.0320, -0.0149, -0.0303, -0.0253, -0.0262,\n",
            "         0.0316,  0.0062, -0.0304, -0.0181, -0.0117,  0.0129,  0.0233, -0.0194,\n",
            "        -0.0275,  0.0041, -0.0107, -0.0198, -0.0210,  0.0024,  0.0306,  0.0076,\n",
            "        -0.0183,  0.0209,  0.0164,  0.0101,  0.0290, -0.0342, -0.0059,  0.0275,\n",
            "         0.0195, -0.0201, -0.0222, -0.0348,  0.0010,  0.0209, -0.0226, -0.0226,\n",
            "         0.0257, -0.0036, -0.0264, -0.0307,  0.0030, -0.0004,  0.0185,  0.0173],\n",
            "       device='cuda:0', requires_grad=True) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
        "\n",
        "print(f\"First Linear weights: {model.linear_relu_stack[0].bias} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEHa5cG0zs8g"
      },
      "source": [
        "## Model layers\n",
        "\n",
        "Let's break down the layers in the FashionMNIST model. To illustrate it, we \n",
        "will take a sample minibatch of 3 images of size **28x28** and see what happens to it as \n",
        "we pass it through the network. \n",
        "\n",
        "让我们分解 FashionMNIST 模型中的层。 为了说明这一点，我们\n",
        "将采用 3 张大小为 **28x28** 的图像作为样本 minibatch，看看它会发生什么\n",
        "我们通过网络传递它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbCjCF5Mzs8g",
        "outputId": "070794b0-8402-4434-8008-b7368ad30fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KgZGzNwazs8g"
      },
      "source": [
        "### nn.Flatten\n",
        "\n",
        "We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained). Each of the pixels are pass to the input layer of the neural network.  \n",
        "\n",
        "<img alt=\"Diagram showing how pixels in image are flatten\" src=\"images/4-model-3.png\" />\n",
        "\n",
        "我们初始化 `nn.Flatten` 层以将每个 2D 28x28 图像转换为 784 个像素值的连续数组（保持小批量维度（dim=0））。 每个像素都传递到神经网络的输入层。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdvG-pJVzs8g",
        "outputId": "7d15ad92-2ecc-47c7-b599-0f837acf1f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruD6Wcnazs8h"
      },
      "source": [
        "### nn.Linear \n",
        "\n",
        "The linear layer is a module that applies a linear transformation on the input using it's stored weights and biases.  The gayscale value of each pixel in the input layer will be connected to neurons in the hidden layer for calculation.    The calculation used for the transformation is ${{weight * input + bias}} $.\n",
        "\n",
        "线性层是一个模块，它使用存储的权重和偏差对输入应用线性变换。 输入层每个像素的gayscale值会连接到隐藏层的神经元进行计算。 用于转换的计算是 ${{weight * input + bias}} $。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "098zPG4Hzs8h",
        "outputId": "70fb67bd-b30c-48ad-d9d2-42297ea3fa73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IiThlAKzs8h"
      },
      "source": [
        "### nn.ReLU\n",
        "\n",
        "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
        "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena. In this model, we use `nn.ReLU` between our linear layers, but there's other activations to introduce non-linearity in your model.\n",
        "\n",
        "The ReLU activation function takes the output from the linear layer calculation and replaces the negative values with zeros.\n",
        "\n",
        "非线性激活是在模型的输入和输出之间创建复杂映射的原因。\n",
        "它们在线性变换之后应用以引入**非线性**，帮助神经网络\n",
        "学习各种各样的现象。 在此模型中，我们在线性层之间使用 `nn.ReLU`，但还有其他激活在您的模型中引入非线性。\n",
        "\n",
        "ReLU 激活函数采用线性层计算的输出并将负值替换为零。\n",
        "\n",
        "Linear output: ${ x = {weight * input + bias}} $.  \n",
        "ReLU:  $f(x)= \n",
        "\\begin{cases}\n",
        "    0, & \\text{if } x < 0\\\\\n",
        "    x, & \\text{if } x\\geq 0\\\\\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_gtwunmzs8h",
        "outputId": "333338fb-1f26-4e26-99bb-d1347bee18a8",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[-0.2530,  0.4927, -0.4120,  0.4664,  0.1131, -0.1839,  0.0128,  0.2858,\n",
            "         -0.1110, -0.4612,  0.0566, -0.6316, -0.0975,  0.0737,  0.0561,  0.0087,\n",
            "         -0.2511,  0.2316,  0.5643, -0.0448],\n",
            "        [-0.0083,  0.6768, -0.5621,  0.2820,  0.3256, -0.0543,  0.3050,  0.2195,\n",
            "         -0.1572, -0.7011, -0.0464, -0.4313, -0.5993, -0.1504,  0.4440, -0.0426,\n",
            "         -0.2414,  0.6542,  0.0531, -0.0562],\n",
            "        [-0.3602,  0.2505, -0.4097,  0.1183,  0.4356, -0.3263, -0.0397,  0.2752,\n",
            "         -0.1516, -0.8040,  0.1152, -0.4537, -0.6999,  0.0703,  0.2744,  0.0032,\n",
            "         -0.2562,  0.4451,  0.3579, -0.0913]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.4927, 0.0000, 0.4664, 0.1131, 0.0000, 0.0128, 0.2858, 0.0000,\n",
            "         0.0000, 0.0566, 0.0000, 0.0000, 0.0737, 0.0561, 0.0087, 0.0000, 0.2316,\n",
            "         0.5643, 0.0000],\n",
            "        [0.0000, 0.6768, 0.0000, 0.2820, 0.3256, 0.0000, 0.3050, 0.2195, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4440, 0.0000, 0.0000, 0.6542,\n",
            "         0.0531, 0.0000],\n",
            "        [0.0000, 0.2505, 0.0000, 0.1183, 0.4356, 0.0000, 0.0000, 0.2752, 0.0000,\n",
            "         0.0000, 0.1152, 0.0000, 0.0000, 0.0703, 0.2744, 0.0032, 0.0000, 0.4451,\n",
            "         0.3579, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ypVzVkzs8h"
      },
      "source": [
        "### nn.Sequential\n",
        "\n",
        "`nn.Sequential` is an ordered \n",
        "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
        "sequential containers to put together a quick network like `seq_modules`.\n",
        "\n",
        "`nn.Sequential` 是有序的\n",
        "模块的容器。 数据按照定义的相同顺序通过所有模块。 您可以使用\n",
        "顺序容器将像 seq_modules 这样的快速网络放在一起。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fVfjburuzs8h"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sa3HJgKzs8i"
      },
      "source": [
        "### nn.Softmax\n",
        "\n",
        "The last linear layer of the neural network returns `logits` - raw values in \\[`-infty`, `infty`], which are passed to the\n",
        "`nn.Softmax` module. The Softmax activation function is used to calculate the probability of the output from the neural network.  It is only used on the output layer of a neural network.  The results are scaled to values \\[0, 1\\] representing the model's predicted densities for each class. `dim` parameter indicates the dimension along which the result values must sum to 1.  The node with the highest probability predicts the desired output.\n",
        "\n",
        "神经网络的最后一个线性层返回 \"logits\"--其值在[`-infty`, `-infty`]之间，这些值被传递到\n",
        "`nn.Softmax`模块。Softmax激活函数用于计算神经网络输出的概率。 它只在神经网络的输出层使用。 结果被缩放为数值\\[0, 1\\]，代表模型对每个类别的预测密度。`dim`参数表示结果值必须和为1的维度。 具有最高概率的节点预测所需的输出。\n",
        "\n",
        "<img alt=\"Diagram shows softmax formula\" src=\"images/4-model-4.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpocaKeczs8i"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MzsfL92zs8i"
      },
      "source": [
        "## Model parameters\n",
        "\n",
        "Many layers inside a neural network are *parameterized*, i.e. have associated weights \n",
        "and biases that are optimized during training. Subclassing `nn.Module` automatically \n",
        "tracks all fields defined inside your model object, and makes all parameters \n",
        "accessible using your model's `parameters()` or `named_parameters()` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
        "\n",
        "神经网络中的许多层是*参数化*的，也就是说，有相关的权重 \n",
        "和偏置，在训练过程中被优化。子类`nn.Module`会自动地 \n",
        "追踪你的模型对象中定义的所有字段，可使用模型的`parameters()`或`named_parameters()`方法访问所有参数。\n",
        "\n",
        "在这个例子中，我们遍历每个参数，并打印其大小和预览其值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvcxl4_Czs8i",
        "outputId": "506b1898-19a9-4f5d-846d-48f268a01675",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0140,  0.0338,  0.0249,  ..., -0.0208, -0.0341,  0.0157],\n",
            "        [-0.0241, -0.0277, -0.0175,  ..., -0.0084,  0.0227, -0.0085]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0243], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0255,  0.0193,  0.0350,  ...,  0.0002, -0.0421,  0.0360],\n",
            "        [-0.0381, -0.0297, -0.0005,  ...,  0.0181, -0.0078, -0.0405]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0236, -0.0282], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0399, -0.0179, -0.0080,  ..., -0.0144,  0.0164,  0.0147],\n",
            "        [ 0.0144,  0.0338, -0.0117,  ...,  0.0165, -0.0147,  0.0139]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0295, -0.0258], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
