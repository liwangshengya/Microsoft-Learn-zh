{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizing the model parameters\n",
        "\n",
        "Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration (called an *epoch*). The model makes a guess about the output, calculates the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in the previous module), and **optimizes** these parameters using gradient descent. \n",
        "\n",
        "现在我们有了模型和数据，是时候通过优化我们的数据参数来训练、验证和测试我们的模型了。 训练模型是一个迭代过程； 在每次迭代中（称为 *epoch*）。 该模型对输出进行猜测，计算其猜测中的误差 (*loss*)，收集关于其参数的误差的导数（正如我们在上一个模块中看到的），并**优化**这些参数 使用梯度下降。\n",
        "\n",
        "## Prerequisite code \n",
        "\n",
        "We will load the code from the previous modules on **Datasets & DataLoaders** and **Build Model**\n",
        "\n",
        "我们将在**数据集和数据加载器**和**建立模型**上加载前面模块的代码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Using downloaded and verified file: data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Using downloaded and verified file: data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Using downloaded and verified file: data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Using downloaded and verified file: data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting hyperparameters \n",
        "\n",
        "Hyperparameters are adjustable parameters that let you control the model optimization process. \n",
        "Different hyperparameter values can impact model training and the level of accuracy.\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        " - **Number of Epochs** - the number times the entire training dataset is pass through the network. \n",
        " - **Batch Size** - the number of data samples seen by the model in each epoch. Iterates are the number of batches needs to compete an epoch.\n",
        " - **Learning Rate** - the size of steps the model match as it searchs for best weights that will produce a higher model accuracy. Smaller values means the model will take a longer time to find the best weights, while larger values may result in the model step over and misses the best weights which yields unpredictable behavior during training.\n",
        "\n",
        "\n",
        "超参数是可调整的参数，可让您控制模型优化过程。\n",
        "不同的超参数值会影响模型训练和准确度水平。\n",
        "\n",
        "我们定义了以下用于训练的超参数：\n",
        "  - **Number of Epochs** - 整个训练数据集通过网络的次数。\n",
        "  - **Batch Size** - 模型在每个时期看到的数据样本数。 迭代次数是竞争一个纪元所需的批次数。\n",
        "  - **Learning Rate** - 模型在搜索可产生更高模型精度的最佳权重时匹配的步长。 较小的值意味着模型将花费更长的时间来找到最佳权重，而较大的值可能会导致模型跳过并错过最佳权重，从而在训练期间产生不可预测的行为。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add an optimization loop\n",
        "\n",
        "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each \n",
        "iteration of the optimization loop is called an **epoch**. \n",
        "\n",
        "Each epoch consists of two main parts:\n",
        " - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
        " - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
        "\n",
        "Let's briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to \n",
        "see the `full-impl-label` of the optimization loop.\n",
        "\n",
        "一旦我们设置了超参数，我们就可以使用优化循环来训练和优化我们的模型。 每个\n",
        "优化循环的迭代称为 **epoch**。\n",
        "\n",
        "每个opoch由两个主要部分组成：\n",
        "  - **训练阶段** - 迭代训练数据集并尝试收敛到最佳参数。\n",
        "  - **验证/测试循环** - 遍历测试数据集以检查模型性能是否正在提高。\n",
        "\n",
        "让我们简要地熟悉一下训练循环中使用的一些概念。 跳转到\n",
        "请参阅优化循环的``full-impl-label``。\n",
        "\n",
        "### Add a loss function\n",
        "\n",
        "当出现一些训练数据时，我们未经训练的网络可能不会给出正确的\n",
        "回答。 **损失函数**衡量得到的结果与目标值的相异程度，\n",
        "它是我们希望在训练过程中最小化的损失函数。 为了计算损失，我们做了一个\n",
        "使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。\n",
        "\n",
        "Common loss functions include:\n",
        "- `nn.MSELoss` (Mean Square Error) used for regression tasks\n",
        "- `nn.NLLLoss` (Negative Log Likelihood) used for classification\n",
        "- `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`\n",
        "\n",
        "We pass our model's output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error.\n",
        "\n",
        "常见的损失函数包括：\n",
        "- `nn.MSELoss`（均方误差）用于回归任务\n",
        "- `nn.NLLLoss`（负对数似然）用于分类\n",
        "- `nn.CrossEntropyLoss` 结合了 `nn.LogSoftmax` 和 `nn.NLLLoss`\n",
        "\n",
        "我们将模型的输出对数传递给 nn.CrossEntropyLoss，它将对对数进行归一化并计算预测误差。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization pass\n",
        "\n",
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent).\n",
        "All optimization logic is encapsulated in  the ``optimizer`` object. Here, we use the SGD optimizer; additionally, there are many different optimizers\n",
        "available in PyTorch such as `ADAM' and 'RMSProp`, that work better for different kinds of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter.\n",
        "\n",
        "\n",
        "\n",
        "优化是在每个训练步骤中调整模型参数以减少模型误差的过程。 **优化算法**定义了这个过程是如何执行的（在这个例子中我们使用随机梯度下降）。\n",
        "所有优化逻辑都封装在``optimizer``对象中。 在这里，我们使用 SGD 优化器； 此外，还有许多不同的优化器\n",
        "在 PyTorch 中可用，例如``ADAM``和``RMSProp``，它们更适用于不同类型的模型和数据。\n",
        "\n",
        "我们通过注册需要训练的模型参数并传入学习率超参数来初始化优化器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        " * Back-propagate the prediction loss with a call to `loss.backwards()`. PyTorch deposits the gradients of the loss w.r.t. each parameter. \n",
        " * Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters by the gradients collected in the backward pass.\n",
        "\n",
        "在训练循环中，优化分三步进行：\n",
        "  * 调用 `optimizer.zero_grad()` 重置模型参数的梯度。 默认情况下渐变相加； 为了防止重复计算，我们在每次迭代时明确地将它们归零。\n",
        "  * 通过调用 `loss.backwards()` 反向传播预测损失。 PyTorch 存储损失 w.r.t. 的梯度。 每个参数。\n",
        "  * 一旦我们有了梯度，我们就调用``optimizer.step()``通过在反向传播中收集的梯度来调整参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full implementation\n",
        "\n",
        "We define `train_loop` that loops over our optimization code, and `test_loop` that \n",
        "evaluates the model's performance against our test data.\n",
        "\n",
        "我们定义了 ``train_loop``来循环我们的优化代码，而 ``test_loop``则根据我们的测试数据来评估模型的性能。\n",
        "对照测试数据评估模型的性能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):        \n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the loss function and optimizer, and pass it to `train_loop` and `test_loop`.\n",
        "Feel free to increase the number of epochs to track the model's improving performance.\n",
        "\n",
        "我们初始化损失函数和优化器，并将其传递给`train_loop`和`test_loop`。\n",
        "随意增加epochs的数量来跟踪模型的改进性能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Feb  5 14:49:04 2023       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\r\n",
            "| N/A   32C    P8    34W / 149W |      7MiB / 11441MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|    0   N/A  N/A      1069      G   /usr/lib/xorg/Xorg                  3MiB |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297787  [    0/60000]\n",
            "loss: 2.293754  [ 6400/60000]\n",
            "loss: 2.282526  [12800/60000]\n",
            "loss: 2.282340  [19200/60000]\n",
            "loss: 2.287089  [25600/60000]\n",
            "loss: 2.272902  [32000/60000]\n",
            "loss: 2.275425  [38400/60000]\n",
            "loss: 2.265132  [44800/60000]\n",
            "loss: 2.245488  [51200/60000]\n",
            "loss: 2.233117  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.6%, Avg loss: 0.035080 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.236138  [    0/60000]\n",
            "loss: 2.222393  [ 6400/60000]\n",
            "loss: 2.186252  [12800/60000]\n",
            "loss: 2.200873  [19200/60000]\n",
            "loss: 2.193500  [25600/60000]\n",
            "loss: 2.165872  [32000/60000]\n",
            "loss: 2.193122  [38400/60000]\n",
            "loss: 2.167030  [44800/60000]\n",
            "loss: 2.151676  [51200/60000]\n",
            "loss: 2.108763  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.3%, Avg loss: 0.033319 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.130956  [    0/60000]\n",
            "loss: 2.094978  [ 6400/60000]\n",
            "loss: 2.028166  [12800/60000]\n",
            "loss: 2.069986  [19200/60000]\n",
            "loss: 2.043037  [25600/60000]\n",
            "loss: 2.007254  [32000/60000]\n",
            "loss: 2.062539  [38400/60000]\n",
            "loss: 2.023079  [44800/60000]\n",
            "loss: 2.018348  [51200/60000]\n",
            "loss: 1.923252  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 43.9%, Avg loss: 0.030808 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.984165  [    0/60000]\n",
            "loss: 1.915824  [ 6400/60000]\n",
            "loss: 1.823707  [12800/60000]\n",
            "loss: 1.897011  [19200/60000]\n",
            "loss: 1.858588  [25600/60000]\n",
            "loss: 1.832745  [32000/60000]\n",
            "loss: 1.902084  [38400/60000]\n",
            "loss: 1.875919  [44800/60000]\n",
            "loss: 1.872555  [51200/60000]\n",
            "loss: 1.733034  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.9%, Avg loss: 0.028302 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.833989  [    0/60000]\n",
            "loss: 1.746030  [ 6400/60000]\n",
            "loss: 1.645118  [12800/60000]\n",
            "loss: 1.737954  [19200/60000]\n",
            "loss: 1.701708  [25600/60000]\n",
            "loss: 1.702900  [32000/60000]\n",
            "loss: 1.762781  [38400/60000]\n",
            "loss: 1.767049  [44800/60000]\n",
            "loss: 1.746727  [51200/60000]\n",
            "loss: 1.592406  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 0.026336 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.709853  [    0/60000]\n",
            "loss: 1.617509  [ 6400/60000]\n",
            "loss: 1.510033  [12800/60000]\n",
            "loss: 1.620824  [19200/60000]\n",
            "loss: 1.580328  [25600/60000]\n",
            "loss: 1.608299  [32000/60000]\n",
            "loss: 1.663435  [38400/60000]\n",
            "loss: 1.690587  [44800/60000]\n",
            "loss: 1.652229  [51200/60000]\n",
            "loss: 1.499496  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.1%, Avg loss: 0.024932 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.618382  [    0/60000]\n",
            "loss: 1.529407  [ 6400/60000]\n",
            "loss: 1.413627  [12800/60000]\n",
            "loss: 1.537707  [19200/60000]\n",
            "loss: 1.493848  [25600/60000]\n",
            "loss: 1.539672  [32000/60000]\n",
            "loss: 1.595456  [38400/60000]\n",
            "loss: 1.638558  [44800/60000]\n",
            "loss: 1.584986  [51200/60000]\n",
            "loss: 1.439390  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.7%, Avg loss: 0.023940 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.549405  [    0/60000]\n",
            "loss: 1.467874  [ 6400/60000]\n",
            "loss: 1.343187  [12800/60000]\n",
            "loss: 1.479312  [19200/60000]\n",
            "loss: 1.432477  [25600/60000]\n",
            "loss: 1.486518  [32000/60000]\n",
            "loss: 1.546793  [38400/60000]\n",
            "loss: 1.599368  [44800/60000]\n",
            "loss: 1.532420  [51200/60000]\n",
            "loss: 1.397420  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.0%, Avg loss: 0.023190 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.494146  [    0/60000]\n",
            "loss: 1.422066  [ 6400/60000]\n",
            "loss: 1.287524  [12800/60000]\n",
            "loss: 1.434927  [19200/60000]\n",
            "loss: 1.386849  [25600/60000]\n",
            "loss: 1.442427  [32000/60000]\n",
            "loss: 1.509750  [38400/60000]\n",
            "loss: 1.568308  [44800/60000]\n",
            "loss: 1.487918  [51200/60000]\n",
            "loss: 1.364272  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.1%, Avg loss: 0.022582 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.446964  [    0/60000]\n",
            "loss: 1.385172  [ 6400/60000]\n",
            "loss: 1.240506  [12800/60000]\n",
            "loss: 1.399133  [19200/60000]\n",
            "loss: 1.351190  [25600/60000]\n",
            "loss: 1.404994  [32000/60000]\n",
            "loss: 1.479754  [38400/60000]\n",
            "loss: 1.541378  [44800/60000]\n",
            "loss: 1.448279  [51200/60000]\n",
            "loss: 1.337112  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.2%, Avg loss: 0.022063 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may have noticed that the model is initially not very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.\n",
        "\n",
        "\n",
        "你可能已经注意到，这个模型最初不是很好（没关系！）。试着在循环中运行更多的 ``epochs ``或将 ``learning_rate``调整到一个更大的数字。也可能是我们选择的模型配置可能不是这种问题的最佳配置（它不是）。以后的课程将更多地深入研究对视觉问题有效的模型形状。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving Models\n",
        "-------------\n",
        "\n",
        "When you are satisfied with the model's performance, you can use `torch.save` to save it. PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted wit the `torch.save` method:\n",
        "\n",
        "当你对模型的性能感到满意时，你可以使用`torch.save`来保存它。PyTorch模型将学到的参数存储在内部状态字典中，称为 ``state_dict``。这些参数可以通过`torch.save`方法进行持久化。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"data/model.pth\")\n",
        "\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-py38_default-py"
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b1e50b9be59f7fbb2b520cd9d0f0de3401ffabf78f05d86508e8222ccb7b50f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
