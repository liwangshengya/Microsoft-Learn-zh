{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words and TF-IDF representations\n",
        "\n",
        "In the previous unit, we have learnt to represent text by numbers. In this unit, we'll explore some of the approaches to feeding variable-length text into a neural network to collapse the input sequence into a fixed-length vector, which can then be used in the classifier.\n",
        "\n",
        "To begin with, let's load our AG News dataset and build the vocabulary, we we have done in the previous unit. To make things shorter, all those operations are combined into `load_dataset` function of the accompanying Python module:\n",
        "\n",
        "在上一单元中，我们学习了用数字表示文本。 在本单元中，我们将探讨一些将可变长度文本输入神经网络以将输入序列折叠为固定长度向量的方法，然后可将其用于分类器。\n",
        "\n",
        "首先，让我们加载我们的 AG News 数据集并构建词汇表，我们在上一个单元中已经完成了。 为了使事情更简短，所有这些操作都被组合到随附的 Python 模块的 `load_dataset` 函数中："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt\n",
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: gensim==3.8.3 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (3.8.3)\nRequirement already satisfied: huggingface==0.0.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 2)) (0.0.1)\nRequirement already satisfied: matplotlib in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.2.1)\nRequirement already satisfied: nltk==3.5 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (3.5)\nRequirement already satisfied: numpy==1.18.5 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 5)) (1.18.5)\nRequirement already satisfied: opencv-python==4.5.1.48 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 6)) (4.5.1.48)\nRequirement already satisfied: Pillow==7.1.2 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 7)) (7.1.2)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (1.0.2)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.5.3)\nCollecting torch==1.8.1\n  Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\nRequirement already satisfied: torchaudio==0.8.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 11)) (0.8.1)\nRequirement already satisfied: torchinfo==0.0.8 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 12)) (0.0.8)\nRequirement already satisfied: torchtext==0.9.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (0.9.1)\nRequirement already satisfied: torchvision==0.9.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 14)) (0.9.1)\nRequirement already satisfied: transformers==4.3.3 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.3.3)\nRequirement already satisfied: smart-open>=1.8.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.9.0)\nRequirement already satisfied: six>=1.5.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.11.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (8.1.3)\nRequirement already satisfied: regex in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (2022.8.17)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (0.14.1)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (4.64.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from scikit-learn->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (2.2.0)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from torch==1.8.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (4.3.0)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.28.1)\nRequirement already satisfied: sacremoses in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.0.53)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.10.3)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.8.0)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (21.3)\nRequirement already satisfied: boto3 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.20.19)\nRequirement already satisfied: boto>=2.32 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (2.49.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.1.1)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (0.10.0)\nRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (0.5.2)\nRequirement already satisfied: botocore<1.24.0,>=1.23.19 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.23.19)\n\u001b[31mERROR: azureml-automl-dnn-nlp 1.45.0 has requirement transformers==4.6.0, but you'll have transformers 4.3.3 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.0\n    Uninstalling torch-1.12.0:\n      Successfully uninstalled torch-1.12.0\nSuccessfully installed torch-1.8.1\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "from torchnlp import *\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size = \",vocab_size)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset...\nBuilding vocab...\nVocab size =  95812\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words text representation\n",
        "\n",
        "Because words represent meaning, sometimes we can figure out the **_meaning of a text_** by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like *weather*, *snow* are likely to indicate *weather forecast*, while words like *stocks*, *dollar* would count towards *financial news*.\n",
        "\n",
        "**Bag of Words** (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occurrences of a word in a given document.\n",
        "\n",
        "因为单词代表意义，有时我们可以通过查看单个单词来弄清楚 **_文本的含义_**，而不管它们在句子中的顺序如何。 例如，在对新闻进行分类时，*天气*、*雪*等词可能表示*天气预报*，而*股票*、*美元*等词则可能表示*财经新闻*。\n",
        "\n",
        "**词袋**（BoW）向量表示是最常用的传统向量表示。 每个词都链接到一个向量索引，向量元素包含一个词在给定文档中出现的次数。\n",
        "\n",
        "<img alt=\"Image showing how a bag of words vector representation is represented in memory.\" src=\"images/3-bow-tfidf-1.png\" align=\"middle\" />\n",
        "\n",
        "> **Note**: You can also think of BoW as a sum of all one-hot-encoded vectors for individual words in the text.\n",
        "\n",
        "Below is an example of how to generate a bag of word representation for a text using vectorization defined previously:\n",
        "\n",
        "> **注意**：您也可以将 BoW 视为文本中单个单词的所有 one-hot-encoded 向量的总和。\n",
        "\n",
        "下面是一个示例，说明如何使用先前定义的矢量化为文本生成词袋表示："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def to_bow(text,bow_vocab_size=vocab_size):\n",
        "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
        "    for i in encode(text):\n",
        "        if i<bow_vocab_size:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(f\"sample text:\\n{train_dataset[0][1]}\")\n",
        "print(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sample text:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n\nBoW vector:\ntensor([0., 0., 2.,  ..., 0., 0., 0.])\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        " > **Note:** Here we are using global `vocab_size` variable to specify default size of the vocabulary. Since often vocabulary size are pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering `vocab_size` value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        " > **注意：** 这里我们使用全局变量 `vocab_size` 来指定词汇表的默认大小。 由于词汇量通常很大，我们可以将词汇量限制为最常用的词。 尝试降低 `vocab_size` 值并运行下面的代码，看看它如何影响准确性。 您应该预料到准确率会有所下降，但不会急剧下降，以代替更高的性能。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training BoW classifier\n",
        "\n",
        "Now that we have learned how to build a Bag-of-Words representation of our text, let's train a classifier on top of it. First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing `bowify` function as `collate_fn` parameter to standard torch `DataLoader`.  The `collate_fn` gives you the ability to apply your own function to the dataset as it's loaded by the Dataloader:\n",
        "\n",
        "现在我们已经学习了如何构建文本的词袋表示，让我们在它之上训练一个分类器。 首先，我们需要转换我们的数据集以进行训练，将所有位置向量表示转换为词袋表示。 这可以通过将 `bowify` 函数作为 `collate_fn` 参数传递给标准 torch `DataLoader` 来实现。 `collate_fn` 使您能够在 Dataloader 加载数据集时将您自己的函数应用于数据集："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np \n",
        "\n",
        "# this collate function gets list of batch_size tuples, and needs to \n",
        "# return a pair of label-feature tensors for the whole minibatch\n",
        "def bowify(b):\n",
        "    return (\n",
        "            torch.LongTensor([t[0]-1 for t in b]),\n",
        "            torch.stack([to_bow(t[1]) for t in b])\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a simple classifier neural network that contains one linear layer. The size of the input vector equals to `vocab_size`, and the output size corresponds to the number of classes (4). Because we are solving a classification task, the final activation function is `LogSoftmax()`.\r\n",
        "\r\n",
        "现在让我们定义一个包含一个线性层的简单分类器神经网络。 输入向量的大小等于`vocab_size`，输出大小对应于类数 (4)。 因为我们正在解决分类任务，所以最终的激活函数是 `LogSoftmax()` 。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will define a standard PyTorch training loop. Because our dataset is quite large, for our teaching purpose we will train only for one epoch, and sometimes even for less than an epoch (specifying the `epoch_size` parameter allows us to limit training). We would also report accumulated training accuracy during training; the frequency of reporting is specified using `report_freq` parameter.\r\n",
        "\r\n",
        "现在我们将定义一个标准的 PyTorch 训练循环。 因为我们的数据集非常大，为了我们的教学目的，我们将只训练一个 epoch，有时甚至少于一个 epoch（指定 `epoch_size` 参数允许我们限制训练）。 我们还将报告训练期间累积的训练准确度； 报告频率使用`report_fre`q参数指定。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,features in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = net(features)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the classifier performs on the training dataset.\r\n",
        "\r\n",
        "让我们看看分类器如何在训练数据集上执行。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch(net,train_loader,epoch_size=15000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.784375\n6400: acc=0.83421875\n9600: acc=0.8473958333333333\n12800: acc=0.860546875\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "(0.025977031000133263, 0.8640724946695096)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The Bag-of-Words approach can be used in the same manner with n-gram tokenizer - only that the vocabulary size would be bigger, and thus the network would have too many parameters. In the next unit, we will see how bigram representation can be used together with embeddings.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Bag-of-Words 方法可以以与 n-gram 分词器相同的方式使用——只是词汇量会更大，因此网络会有太多参数。 在下一个单元中，我们将看到如何将二元表示与嵌入一起使用。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Term Frequency / Inverse Document Frequency:  TF-IDF\n",
        "\n",
        "In BoW representation, word occurrences are evenly weighted, regardless of the word itself. However, it is clear that frequent words, such as *'a'*, *'in'*, *'the'* etc. are much less important for the classification, than specialized terms. In fact, in most NLP tasks some words are more relevant than others.\n",
        "\n",
        "**TF-IDF** stands for **term frequency–inverse document frequency**. It is a variation of bag of words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the **_frequency of word occurrence_** in the corpus.\n",
        "\n",
        "The formula to calculate TF-IDF is:  $w_{ij} = tf_{ij}\\times\\log({N\\over df_i})$\n",
        "\n",
        "Here's the meaning of each parameter in the formula:\n",
        "* $i$ is the word \n",
        "* $j$ is the document\n",
        "* $w_{ij}$ is the weight or the importance of the word in the document\n",
        "* $tf_{ij}$ is the number of occurrences of the word $i$ in the document $j$, i.e. the BoW value we have seen before\n",
        "* $N$ is the number of documents in the collection\n",
        "* $df_i$ is the number of documents containing the word $i$ in the whole collection\n",
        "\n",
        "\n",
        "在 BoW 表示中，单词出现的权重是均匀的，与单词本身无关。 然而，很明显，与专业术语相比，频繁出现的词，如 *'a'*、*'in'*、*'the'* 等，对于分类而言重要性要低得多。 事实上，在大多数 NLP 任务中，有些词比其他词更相关。\n",
        "\n",
        "**TF-IDF** 代表 **term frequency–inverse document frequency**。 它是词袋的变体，其中使用浮点值代替二进制 0/1 值指示文档中单词的出现，这与 **_frequency of word occurrence_** in 语料库。\n",
        "\n",
        "TF-IDF的计算公式为：$w_{ij} = tf_{ij}\\times\\log({N\\over df_i})$\n",
        "\n",
        "下面是公式中各个参数的含义：\n",
        "* $i$ 是单词\n",
        "* $j$ 是文档\n",
        "* $w_{ij}$ 是单词在文档中的权重或重要性\n",
        "* $tf_{ij}$是单词$i$在文档$j$中出现的次数，即我们之前看到的BoW值\n",
        "* $N$ 是集合中文档的数量\n",
        "* $df_i$ 是整个集合中包含单词 $i$ 的文档数\n",
        "\n",
        "<img alt=\"Diagram showing table representing word frequent in documents.\" src=\"images/3-bow-tfidf-2.png\" align=\"middle\" />\n",
        "\n",
        "TF-IDF value $w_{ij}$ increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in *every* document in the collection, $df_i=N$, and $w_{ij}=0$, and those terms would be completely disregarded.\n",
        "\n",
        "First, let's compute document frequency $df_i$ for each word $i$. We can represent it as tensor of size `vocab_size`. We will limit the number of documents to $N=1000$ to speed up processing. For each input sentence, we compute the set of words (represented by their numbers), and increase the corresponding counter:\n",
        "\n",
        "TF-IDF 值 $w_{ij}$ 与一个词在文档中出现的次数成正比增加，并被语料库中包含该词的文档数抵消，这有助于调整某些词出现的事实 比其他人更频繁。 例如，如果单词出现在集合中的*每个*文档中，$df_i=N$ 和 $w_{ij}=0$，那么这些术语将被完全忽略。\n",
        "\n",
        "首先，让我们计算每个单词 $i$ 的文档频率 $df_i$。 我们可以将其表示为大小为`vocab_size`的张量。 我们会将文档数量限制为 $N=1000$ 以加快处理速度。 对于每个输入句子，我们计算单词集（由它们的数字表示），并增加相应的计数器："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1000\n",
        "df = torch.zeros(vocab_size)\n",
        "for _,line in train_dataset[:N]:\n",
        "    for i in set(encode(line)):\n",
        "        df[i] += 1"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have document frequencies for each word, we can define `tf_idf` function that will take a string, and produce TF-IDF vector. We will use `to_bow` defined above to calculate term frequency vector, and multiply it by inverse document frequency of the corresponding term. Remember that all tensor operations are element-wise, which allows us to implement the whole computation as a tensor formula:\r\n",
        "\r\n",
        "现在我们有了每个单词的文档频率，我们可以定义 `tf_idf` 函数，该函数将获取一个字符串，并生成 TF-IDF 向量。 我们将使用上面定义的 `to_bow` 来计算术语频率向量，并将其乘以相应术语的逆文档频率。 请记住，所有张量运算都是按元素进行的，这使我们可以将整个计算实现为张量公式："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(s):\n",
        "    bow = to_bow(s)\n",
        "    return bow*torch.log((N+1)/(df+1))\n",
        "\n",
        "print(tf_idf(train_dataset[0][1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([0.0000, 0.0000, 0.0363,  ..., 0.0000, 0.0000, 0.0000])\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You may have noticed that we used a slightly different formula for TF-IDF, namely $\\log({N+1\\over df_i+1})$ instead of $\\log({N\\over df_i})$. This yields similar results, but prevents division by 0 in those cases when $df_i=0$.\n",
        "\n",
        "Even though TF-IDF representation calculates different weights to different words according to their importance, it is unable to correctly capture the meaning, largely because the order of words in the sentence is still not taken into account. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously”. We will learn in the later units how to capture contextual information from text using language modeling.\n",
        "\n",
        "> 你可能已经注意到我们对 TF-IDF 使用了一个稍微不同的公式，即 $\\log({N+1\\over df_i+1})$ 而不是 $\\log({N\\over df_i})$。 这会产生类似的结果，但在 $df_i=0$ 的情况下会阻止除以 0。\n",
        "\n",
        "即使 TF-IDF 表示根据重要性对不同的词计算不同的权重，它也无法正确地捕捉含义，很大程度上是因为仍然没有考虑句子中词的顺序。 正如著名语言学家 J. R. Firth 在 1935 年所说的那样，“一个词的完整含义总是与上下文相关的，离开上下文对意义的研究是不能认真对待的”。 我们将在后面的单元中学习如何使用语言建模从文本中捕获上下文信息。\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
    },
    "kernelspec": {
      "name": "conda-env-azureml_py38_PT_and_TF-py",
      "language": "python",
      "display_name": "azureml_py38_PT_and_TF"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-azureml_py38_PT_and_TF-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}