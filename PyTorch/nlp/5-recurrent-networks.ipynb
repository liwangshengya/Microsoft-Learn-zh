{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent neural networks\n",
        "\n",
        "In the previous module, we have been using rich **_semantic representations of text_**, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it does not take into account the **order** of words, because aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.\n",
        "\n",
        "To capture the meaning of text sequence, we need to use another neural network architecture, which is called a **recurrent neural network**, or RNN. In RNN, we pass our sentence through the network one word vector from a news article sequence at a time, and the network produces some **state**, which we then pass to the network again with the next one word vector from the sequence.  RNN storing a \"memory\" of the previous in the state, helps the network understand the **_context of the sentence_** to be able to predict the network word in the sequence.\n",
        "\n",
        "在上一个模块中，我们一直在使用丰富的 **_文本_** 语义表示，以及在嵌入之上的简单线性分类器。 该架构所做的是捕获句子中单词的聚合含义，但它没有考虑单词的**顺序**，因为嵌入之上的聚合操作从原始文本中删除了该信息。 由于这些模型无法对词序进行建模，因此它们无法解决更复杂或模棱两可的任务，例如文本生成或问题回答。\n",
        "\n",
        "为了捕捉文本序列的含义，我们需要使用另一种神经网络架构，称为**递归神经网络**，或 RNN。 在 RNN 中，我们一次将新闻文章序列中的一个词向量通过网络传递给我们的句子，网络产生一些**状态**，然后我们将其与序列中的下一个词向量再次传递给网络 . RNN 在状态中存储前一个的“记忆”，帮助网络理解 **_句子的上下文_** 以便能够预测序列中的网络词。\n",
        "\n",
        "<img alt=\"Image showing an example recurrent neural network generation.\" src=\"images/5-recurrent-networks-1.png\" align=\"middle\" />\n",
        "\n",
        "- Given the input sequence of word vectors $X_0,\\dots,X_n$, RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using back propagation. \n",
        "- Each network block takes a pair $(X_i,h_i)$ as an input, and produces $h_{i+1}$ as a result. \n",
        "- Final state $h_n$ or output $y$ goes into a linear classifier to produce the result. \n",
        "- All network blocks share the same weights, and are trained end-to-end using one backpropagation pass.\n",
        "\n",
        "The hidden cell containing the current and prior state is calculated with the following formula:\n",
        "\n",
        "- $h(t) = {\\tanh(W_{h}h_{t-1} + W_{x}x_{t} + B_{h}) }$ \n",
        "- $y(t) = {  W_{y}h_{t} + B_{y} }$ \n",
        "- Tanh is hyperbolic tangent function, which is defined as ${\\tanh(x)} = {\\large \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}} $\n",
        "\n",
        "At each network block, weights $W_{x}$ are applied to the numeric word vector input value; applying the previous hidden state $W_{h}$; and the  final state $W_{y}$. The ${tanh}$ activation function is applied to the hidden layer to produce values between $[-1,1]$.\n",
        "\n",
        "Because state vectors $h_0,\\dots,h_n$ are passed through the network, it is able to learn the sequential dependencies between words. For example, when the word **_not_** appears somewhere in the sequence, it can learn to negate certain elements within the state vector, resulting in negation.  \n",
        "\n",
        "Let's see how recurrent neural networks can help us classify our news dataset.\n",
        "\n",
        "\n",
        "- 给定词向量 $X_0,\\dots,X_n$ 的输入序列，RNN 创建一个神经网络块序列，并使用反向传播端到端地训练这个序列。\n",
        "- 每个网络块将一对 $(X_i,h_i)$ 作为输入，并产生 $h_{i+1}$ 作为结果。\n",
        "- 最终状态 $h_n$ 或输出 $y$ 进入线性分类器以产生结果。\n",
        "- 所有网络块共享相同的权重，并使用一个反向传播通道进行端到端训练。\n",
        "\n",
        "包含当前和先前状态的隐藏单元使用以下公式计算：\n",
        "\n",
        "- $h(t) = {\\tanh(W_{h}h_{t-1} + W_{x}x_{t} + B_{h}) }$\n",
        "- $y(t) = { W_{y}h_{t} + B_{y} }$\n",
        "- Tanh 是双曲正切函数，定义为 ${\\tanh(x)} = {\\large \\frac{e^{x} - e^{-x}}{e^{x} + e^{- x}}} $\n",
        "\n",
        "在每个网络块，权重 $W_{x}$ 应用于数字词向量输入值； 应用先前的隐藏状态 $W_{h}$； 和最终状态 $W_{y}$。 ${tanh}$ 激活函数应用于隐藏层以产生 $[-1,1]$ 之间的值。\n",
        "\n",
        "因为状态向量 $h_0,\\dots,h_n$ 通过网络传递，所以它能够学习单词之间的顺序依赖关系。 例如，当单词 **_not_** 出现在序列的某处时，它可以学习对状态向量内的某些元素取反，从而产生否定。\n",
        "\n",
        "让我们看看递归神经网络如何帮助我们对新闻数据集进行分类。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchinfo import summary\n",
        "from torchnlp import *\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
        "vocab_size = len(vocab)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset...\nBuilding vocab...\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN classifier\n",
        "\n",
        "In the case of simple RNN, each recurrent unit is a simple linear network, which takes concatenated input vector and state vector, and produce a new state vector. PyTorch represents this unit with `RNNCell` class, and a networks of each cells - as `RNN` layer.\n",
        "\n",
        "To define an RNN classifier, we will first apply an embedding layer to lower the dimensionality of input vocabulary, and then have a RNN layer on top of it: \n",
        "\n",
        "在简单 RNN 的情况下，每个循环单元都是一个简单的线性网络，它采用连接的输入向量和状态向量，并产生一个新的状态向量。 PyTorch 用`RNNCell`类和每个单元的网络表示这个单元——作为`RNN`层。\n",
        "\n",
        "要定义一个 RNN 分类器，我们首先应用一个嵌入层来降低输入词汇的维度，然后在它之上有一个 RNN 层："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.embedding(x)\n",
        "        x,h = self.rnn(x)\n",
        "        return self.fc(x.mean(dim=1))"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** We use untrained embedding layer here for simplicity, but for even better results we can use pre-trained embedding layer with Word2Vec or GloVe embeddings, as described in the previous unit. For better understanding, you might want to adapt this code to work with pre-trained embeddings.\n",
        "\n",
        "In our case, we will use padded data loader, so each batch will have a number of padded sequences of the same length. RNN layer will take the sequence of embedding tensors, and produce two outputs:\n",
        "\n",
        "* The `input` to the embedding layer is the word sequence or news article\n",
        "* The `embedding layer` output contains the vector index value in vocab for each word in the sequence\n",
        "* $x$ is a sequence of RNN cell outputs at each step.  \n",
        "* $h$ is a final `hidden state` for the last element of the sequence.  Each RNN hidden layer stores the prior word in the sequence and the current as each word in the sequence is passed through the layers.\n",
        "\n",
        "We then apply a fully-connected linear classifier to get the probability for number of classes.\n",
        "\n",
        "> **Note:** RNNs are quite difficult to train, because once the RNN cells are unrolled along the sequence length, the resulting number of layers involved in back propagation is quite large. Thus we need to select small learning rate, and train the network on larger dataset to produce good results. It can take quite a long time, so using GPU is preferred.\n",
        "\n",
        "\n",
        "> **注意：** 为简单起见，我们在这里使用未经训练的嵌入层，但为了获得更好的结果，我们可以使用带有 Word2Vec 或 GloVe 嵌入的预训练嵌入层，如上一单元所述。 为了更好地理解，您可能希望调整此代码以使用预训练的嵌入。\n",
        "\n",
        "在我们的例子中，我们将使用填充数据加载器，因此每个批次都会有许多相同长度的填充序列。 RNN 层将采用嵌入张量的序列，并产生两个输出：\n",
        "\n",
        "* 嵌入层的“输入”是词序列或新闻文章\n",
        "* `embedding layer` 输出包含序列中每个单词的 vocab 中的向量索引值\n",
        "* $x$ 是每一步的 RNN 单元输出序列。\n",
        "* $h$ 是序列最后一个元素的最终“隐藏状态”。 每个 RNN 隐藏层存储序列中的前一个单词和当前单词，因为序列中的每个单词都通过这些层。\n",
        "\n",
        "然后我们应用一个完全连接的线性分类器来获得类数的概率。\n",
        "\n",
        "> **注意：** RNN 很难训练，因为一旦 RNN 单元沿着序列长度展开，反向传播涉及的层数就会非常大。 因此我们需要选择较小的学习率，并在较大的数据集上训练网络以产生良好的结果。 这可能需要相当长的时间，因此首选使用 GPU。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
        "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
        "train_epoch(net,train_loader, lr=0.001)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.3340625\n6400: acc=0.4190625\n9600: acc=0.48\n12800: acc=0.52171875\n16000: acc=0.5511875\n19200: acc=0.5776041666666667\n22400: acc=0.5994196428571429\n25600: acc=0.6203125\n28800: acc=0.6385416666666667\n32000: acc=0.65396875\n35200: acc=0.6679829545454545\n38400: acc=0.6797395833333333\n41600: acc=0.6912740384615385\n44800: acc=0.7015848214285715\n48000: acc=0.7107291666666666\n51200: acc=0.71904296875\n54400: acc=0.72625\n57600: acc=0.7325173611111111\n60800: acc=0.7388322368421053\n64000: acc=0.74484375\n67200: acc=0.7508630952380952\n70400: acc=0.7560795454545455\n73600: acc=0.7609510869565217\n76800: acc=0.7654036458333333\n80000: acc=0.7696625\n83200: acc=0.7738341346153846\n86400: acc=0.7775231481481482\n89600: acc=0.780625\n92800: acc=0.784084051724138\n96000: acc=0.7874895833333333\n99200: acc=0.790241935483871\n102400: acc=0.793046875\n105600: acc=0.7957102272727272\n108800: acc=0.7985477941176471\n112000: acc=0.8011696428571429\n115200: acc=0.8034722222222223\n118400: acc=0.8055827702702703\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "(0.03306732177734375, 0.806625)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load the test dataset to evaluate the trained RNN model.  We'll be using the 4 different classes of the news categories to map the predicted output with the targeted label.\r\n",
        "\r\n",
        "现在，让我们加载测试数据集来评估经过训练的 RNN 模型。 我们将使用 4 个不同类别的新闻类别将预测输出与目标标签进行映射。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'class map: {classes}')\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "class map: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we evaluate the model, we'll extract the padded vector dataset from the dataloader.  We will use the vocab.itos function to convert the numeric index to the word it matches in the vocabulary.  When conversion from numeric to string happens for padded vectors, the '0' values are set to a special character `<unk>` as an unknown identifier. So, the character needs to be removed, depending on the unknown values from the padded zeros.\n",
        "\n",
        "Finally, we’ll run the model with our test dataset to verify if the expected output matched the predicted.\n",
        "\n",
        "在我们评估模型之前，我们将从数据加载器中提取填充的矢量数据集。 我们将使用 vocab.itos 函数将数字索引转换为它在词汇表中匹配的单词。 当从数字到字符串的转换为填充向量时，将“ 0”值设置为特殊字符`unk`作为未知标识符。 因此，需要删除该字符，具体取决于填充零中的未知值。\n",
        "\n",
        "最后，我们将使用我们的测试数据集运行模型以验证预期输出是否与预测相符。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (target, data) in enumerate(test_loader):\n",
        "        \n",
        "        word_lookup = [vocab.itos[w] for w in data[batch_idx]]\n",
        "        unknow_vals = {'<unk>'}\n",
        "        word_lookup = [ele for ele in word_lookup if ele not in unknow_vals]\n",
        "        print('Input text:\\n {}\\n'.format(word_lookup))\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pred = net(data)\n",
        "        print(torch.argmax(pred[batch_idx]))\n",
        "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(target[batch_idx], classes[target[batch_idx]]))\n",
        "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),classes[pred[0].argmax(0)]))\n",
        "        break"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Input text:\n ['business', 'bush', 'administration', 'washington', 'post', 'business', 'columnist', 'steven', 'pearlstein', 'will', 'be', 'online', 'to', 'discuss', 'his', 'latest', 'column', ',', 'which', 'looks', 'at', 'the', 'bush', 'administration', \"'\", 's', 'plans', 'for', 'social', 'security', 'and', 'health', 'care', '.']\n\ntensor(2)\nActual:\nvalue=2, class_name= Business\n\nPredicted:\nvalue=2, class_name= Business\n\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short Term Memory (LSTM)\n",
        "\n",
        "One of the main problems of classical RNNs is the so-called **vanishing gradients** problem. Because RNNs are trained end-to-end in one back-propagation pass, it is having hard times propagating error to the first layers of the network, and thus the network cannot learn relationships between distant tokens. The gradient helps in adjusting the weights during back-progagation to achieve better accuracy and reduce the error margin.  If the weights are too small the network does not learn.  Since the gradient decreases during back-propagation in RNNs, the network does not learn the initial inputs in the network.  In other ways, the network \"forgets\" the earlier word inputs.\n",
        "\n",
        "One of the ways to avoid this problem is to introduce **explicit state management** by using so called **gates**. There are two most known architectures of this kind: **Long Short Term Memory** (LSTM) and **Gated Relay Unit** (GRU).\n",
        "\n",
        "经典 RNN 的主要问题之一是所谓的**梯度消失**问题。 由于 RNN 在一次反向传播过程中进行端到端训练，因此很难将错误传播到网络的第一层，因此网络无法学习远距离标记之间的关系。 梯度有助于在反向传播期间调整权重，以实现更好的准确性并减少误差范围。 如果权重太小，网络就不会学习。 由于在 RNN 的反向传播过程中梯度下降，网络不会学习网络中的初始输入。 在其他方面，网络“忘记”了较早的单词输入。\n",
        "\n",
        "避免此问题的方法之一是通过使用所谓的**门**引入**显式状态管理**。 有两种最著名的此类架构：**长短期记忆** (LSTM) 和 **门控中继单元** (GRU)。\n",
        "\n",
        "<img alt=\"Image showing an example long short term memory cell\" src=\"images/5-recurrent-networks-2.png\" align=\"middle\" />\n",
        "\n",
        "\n",
        "LSTM Network is organized in a manner similar to RNN, but there are two states that are being passed from layer to layer: `actual state` $c$, and `hidden vector` $h$. \n",
        "- At each unit, hidden vector $h_i$ is concatenated with input $x_i$, and they `control what happens to the state $c$ via **gates**. \n",
        "- Each gate is a neural network with `sigmoid` (${\\sigma}$) activation (output in the range $[0,1]$), which can be thought of as bitwise mask when multiplied by the state vector. \n",
        "\n",
        "There are the following gates (from left to right on the picture above):\n",
        "* **forget gate** takes hidden vector and determines, which components of the vector $c$ we need to forget, and which to pass through. The gate determines which words are not important and sigmod values close to zero need to be thrown out.  The formula is $f_t = \\sigma(W_f  * [x_t + h_{t-1}] + b_f)$.\n",
        "* **input gate** takes some information from the input and hidden vector, and inserts it into state. The formula for the input gate is a product of the new information $i_t = \\sigma(W_i  * [x_t + h_{t-1}] + b_f)$ and the hidden  $\\tilde{C_t} = \\tanh(W_f  * [x_t + h_{t-1}] + b_f)$\n",
        "* **output gate** transforms state via some linear layer with `tanh` activation, then selects some of its components using hidden vector $h_i$ to produce new state $c_{i+1}$.  The formula for the input gate is $o_t = \\sigma(W_o  * [x_t + h_{t-1}] + b_f)$ and the hidden is ${h_t} = {o_t} * \\tanh(C_t) $\n",
        "* **cell state**  takes a product of the hidden state and the forget gate.  Then sums value with the product of the input gate and output gate. $C_t = f_t \\circ C_{t-1} + i_t \\circ \\tilde{C_t}$ \n",
        "\n",
        "LSTM 网络的组织方式类似于 RNN，但有两种状态在层与层之间传递：“实际状态”$c$ 和“隐藏向量”$h$。\n",
        "- 在每个单元，隐藏向量 $h_i$ 与输入 $x_i$ 连接，它们通过 **门** 控制状态 $c$ 发生的事情。\n",
        "- 每个门都是一个具有 `sigmoid` (${\\sigma}$) 激活的神经网络（输出在 $[0,1]$ 范围内），当与状态向量相乘时可以将其视为按位掩码。\n",
        "\n",
        "有以下门（上图从左到右）：\n",
        "* **遗忘门** 采用隐藏向量并确定我们需要遗忘向量 $c$ 的哪些分量，以及传递哪些分量。 门确定哪些词不重要，并且需要丢弃接近零的 sigmod 值。 公式是 $f_t = \\sigma(W_f * [x_t + h_{t-1}] + b_f)$。\n",
        "* **输入门** 从输入和隐藏向量中获取一些信息，并将其插入状态。 输入门的公式是新信息 $i_t = \\sigma(W_i * [x_t + h_{t-1}] + b_f)$ 和隐藏 $\\tilde{C_t} = \\tanh(W_f * [x_t + h_{t-1}] + b_f)$\n",
        "* **输出门** 通过一些具有“tanh”激活的线性层转换状态，然后使用隐藏向量 $h_i$ 选择其某些组件以生成新状态 $c_{i+1}$。 输入门的公式是 $o_t = \\sigma(W_o * [x_t + h_{t-1}] + b_f)$ 隐藏层是 ${h_t} = {o_t} * \\tanh(C_t) $\n",
        "* **细胞状态**采用隐藏状态和遗忘门的乘积。 然后将值与输入门和输出门的乘积相加。 $C_t = f_t \\circ C_{t-1} + i_t \\circ \\tilde{C_t}$\n",
        "\n",
        "Components of the state $c$ can be thought of as some flags that can be switched on and off. For example, when we encounter a name *'Alice'* in the sequence, we may want to assume that it refers to female character, and raise the flag in the state that we have female noun in the sentence. When we further encounter phrases *and Tom*, we will raise the flag that we have plural noun. Thus by manipulating state we can supposedly keep track of grammatical properties of sentence parts.\n",
        "\n",
        "> **Note**: A great resource for understanding internals of LSTM is this great article \"Understanding LSTM Networks\" by Christopher Olah.\n",
        "\n",
        "While internal structure of LSTM cell may look complex, PyTorch hides this implementation inside the `LSTMCell` class, and provides a `LSTM` object to represent the whole LSTM layer. Thus, implementation of LSTM classifier will be pretty similar to the simple RNN which we have seen above:\n",
        "\n",
        "状态 $c$ 的组件可以被认为是一些可以打开和关闭的标志。 例如，当我们在序列中遇到一个名字 *'Alice'* 时，我们可能想假设它指的是女性角色，并在句子中有女性名词的状态下举起旗帜。 当我们进一步遇到短语 * 和 Tom * 时，我们将举起我们拥有复数名词的标志。 因此，通过操纵状态，我们可以假设跟踪句子部分的语法属性。\n",
        "\n",
        "> **注意**：了解 LSTM 内部结构的重要资源是 Christopher Olah 撰写的这篇很棒的文章“[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)”。([中文版翻译](https://zhuanlan.zhihu.com/p/84333843))\n",
        "\n",
        "虽然 LSTM 单元的内部结构可能看起来很复杂，但 PyTorch 将此实现隐藏在 `LSTMCell` 类中，并提供了一个 `LSTM` 对象来表示整个 LSTM 层。 因此，LSTM 分类器的实现将与我们上面看到的简单 RNN 非常相似："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
        "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.embedding(x)\n",
        "        x,(h,c) = self.rnn(x)\n",
        "        return self.fc(h[-1])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train our network. Note that training LSTM is also quite slow, and you may not seem much raise in accuracy in the beginning of training. Also, you may need to play with `lr` learning rate parameter to find the learning rate that results in reasonable training speed.\r\n",
        "\r\n",
        "现在让我们训练我们的网络。 请注意，训练 LSTM 也很慢，并且在训练开始时您的准确性似乎没有太大提高。 此外，您可能需要使用`lr`学习率参数来找到能够产生合理训练速度的学习率。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = LSTMClassifier(vocab_size,64,32,len(classes)).to(device)\n",
        "train_epoch(net,train_loader, lr=0.001)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.2509375\n6400: acc=0.264375\n9600: acc=0.27979166666666666\n12800: acc=0.3003125\n16000: acc=0.32375\n19200: acc=0.3491666666666667\n22400: acc=0.3639732142857143\n25600: acc=0.38234375\n28800: acc=0.39788194444444447\n32000: acc=0.41425\n35200: acc=0.42798295454545454\n38400: acc=0.44244791666666666\n41600: acc=0.4593269230769231\n44800: acc=0.47261160714285716\n48000: acc=0.4881875\n51200: acc=0.50271484375\n54400: acc=0.5168933823529411\n57600: acc=0.5295486111111111\n60800: acc=0.5411019736842105\n64000: acc=0.552671875\n67200: acc=0.5638244047619048\n70400: acc=0.5748153409090909\n73600: acc=0.585\n76800: acc=0.59453125\n80000: acc=0.60405\n83200: acc=0.6122956730769231\n86400: acc=0.6208564814814815\n89600: acc=0.6287053571428571\n92800: acc=0.6360991379310345\n96000: acc=0.6432916666666667\n99200: acc=0.6497983870967742\n102400: acc=0.65572265625\n105600: acc=0.6617708333333333\n108800: acc=0.6674264705882353\n112000: acc=0.6726785714285715\n115200: acc=0.6782986111111111\n118400: acc=0.6834375\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "(0.04554988606770834, 0.686025)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packed sequences\n",
        "\n",
        "In our example, we had to pad all sequences in the minibatch with zero vectors. While it results in some memory waste, with RNNs it is more critical that additional RNN cells are created for the padded input items, which take part in training, yet do not carry any important input information. It would be much better to train RNN only to the actual sequence size.\n",
        "\n",
        "To do that, a special format of padded sequence storage is introduced in PyTorch. Suppose we have input padded minibatch which looks like this:\n",
        "\n",
        "在我们的示例中，我们必须用零向量填充小批量中的所有序列。 虽然它会导致一些内存浪费，但对于 RNN，更重要的是为填充的输入项创建额外的 RNN 单元，这些输入项参与训练，但不携带任何重要的输入信息。 仅将 RNN 训练到实际序列大小会好得多。\n",
        "\n",
        "为此，PyTorch 中引入了一种特殊的填充序列存储格式。 假设我们有输入填充的小批量，如下所示：\n",
        "\n",
        "```\n",
        "[[1,2,3,4,5],\n",
        " [6,7,8,0,0],\n",
        " [9,0,0,0,0]]\n",
        "```\n",
        "Here 0 represents padded values, and the actual length vector of input sequences is `[5,3,1]`.\n",
        "\n",
        "In order to effectively train RNN with padded sequence, we want to begin training first group of RNN cells with large minibatch (`[1,6,9]`), but then end processing of third sequence, and continue training with shorted minibatches (`[2,7]`, `[3,8]`), and so on. Thus, packed sequence is represented as one vector - in our case `[1,6,9,2,7,3,8,4,5]`, and length vector (`[5,3,1]`), from which we can easily reconstruct the original padded minibatch.\n",
        "\n",
        "To produce packed sequence, we can use `torch.nn.utils.rnn.pack_padded_sequence` function. All recurrent layers, including RNN, LSTM and GRU, support packed sequences as input, and produce packed output, which can be decoded using `torch.nn.utils.rnn.pad_packed_sequence`.\n",
        "\n",
        "To be able to produce packed sequence, we need to pass length vector to the network, and thus we need a different function to prepare minibatches:\n",
        "\n",
        "这里0表示填充值，输入序列的实际长度向量为`[5,3,1]`。\n",
        "\n",
        "为了有效地训练带有填充序列的 RNN，我们想要开始训练第一组具有大 minibatch (`[1,6,9]`) 的 RNN 单元，然后结束第三个序列的处理，并继续使用较短的 minibatch 进行训练 ( `[2,7]`、`[3,8]`)，等等。 因此，打包序列表示为一个向量 - 在我们的例子中为`[1,6,9,2,7,3,8,4,5]`和长度向量 (`[5,3,1]`)， 从中我们可以很容易地重建原始的填充小批量。\n",
        "\n",
        "要生成打包序列，我们可以使用 `torch.nn.utils.rnn.pack_padded_sequence` 函数。 所有循环层，包括 RNN、LSTM 和 GRU，都支持打包序列作为输入，并产生打包输出，可以使用 `torch.nn.utils.rnn.pad_packed_sequence` 进行解码。\n",
        "\n",
        "为了能够生成打包序列，我们需要将长度向量传递给网络，因此我们需要一个不同的函数来准备小批量："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_length(b):\n",
        "    # build vectorized sequence\n",
        "    v = [encode(x[1]) for x in b]\n",
        "    # compute max length of a sequence in this minibatch and length sequence itself\n",
        "    len_seq = list(map(len,v))\n",
        "    l = max(len_seq)\n",
        "    return ( # tuple of three tensors - labels, padded features, length sequence\n",
        "        torch.LongTensor([t[0]-1 for t in b]),\n",
        "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v]),\n",
        "        torch.tensor(len_seq)\n",
        "    )\n",
        "\n",
        "train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)\n",
        "test_loader_len = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual network would be very similar to `LSTMClassifier` above, but `forward` pass will receive both padded minibatch and the vector of sequence lengths. After computing the embedding, we compute packed sequence, pass it to LSTM layer, and then unpack the result back.\n",
        "\n",
        "> **Note**: We actually do not use unpacked result `x`, because we use output from the hidden layers in the following computations. Thus, we can remove the unpacking altogether from this code. The reason we place it here is for you to be able to modify this code easily, in case you should need to use network output in further computations.\n",
        "\n",
        "实际网络与上面的`LSTMClassifier`非常相似，但`前向`传递将同时接收填充的小批量和序列长度向量。 计算嵌入后，我们计算打包序列，将其传递给 LSTM 层，然后将结果解包回来。\n",
        "\n",
        "> **注意**：我们实际上不使用解压缩结果 `x`，因为我们在以下计算中使用隐藏层的输出。 因此，我们可以从此代码中完全删除解包。 我们将它放在这里的原因是为了让您能够轻松修改此代码，以防您需要在进一步的计算中使用网络输出。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMPackClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
        "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.embedding(x)\n",
        "        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
        "        _,(h,c) = self.rnn(pad_x)\n",
        "        return self.fc(h[-1])"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train our network with the padded sequence:\r\n",
        "\r\n",
        "现在让我们用填充序列训练我们的网络："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = LSTMPackClassifier(vocab_size,64,32,len(classes)).to(device)\n",
        "train_epoch_emb(net,train_loader_len, lr=0.001,use_pack_sequence=True)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.29875\n6400: acc=0.359375\n9600: acc=0.41458333333333336\n12800: acc=0.462890625\n16000: acc=0.50425\n19200: acc=0.5391666666666667\n22400: acc=0.5695089285714285\n25600: acc=0.5971875\n28800: acc=0.6198611111111111\n32000: acc=0.6396875\n35200: acc=0.6576988636363637\n38400: acc=0.6736197916666666\n41600: acc=0.6871394230769231\n44800: acc=0.6997098214285714\n48000: acc=0.70975\n51200: acc=0.71970703125\n54400: acc=0.727922794117647\n57600: acc=0.7359375\n60800: acc=0.7429934210526316\n64000: acc=0.74978125\n67200: acc=0.7558928571428571\n70400: acc=0.7613920454545454\n73600: acc=0.7664402173913043\n76800: acc=0.7716536458333333\n80000: acc=0.7760125\n83200: acc=0.7801802884615384\n86400: acc=0.7843981481481481\n89600: acc=0.788125\n92800: acc=0.7916810344827586\n96000: acc=0.79515625\n99200: acc=0.7982762096774193\n102400: acc=0.801162109375\n105600: acc=0.8040151515151515\n108800: acc=0.8068198529411764\n112000: acc=0.8093125\n115200: acc=0.8118055555555556\n118400: acc=0.8141807432432432\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "(0.029937251790364584, 0.8152833333333334)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** You may have noticed the parameter `use_pack_sequence` that we pass to the training function. Currently, `pack_padded_sequence` function requires length sequence tensor to be on CPU device, and thus training function needs to avoid moving the length sequence data to GPU when training. You can look into implementation of `train_epoch_emb` helper function in the `torchnlp.py` file located in the local directory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **注意：**您可能已经注意到我们传递给训练函数的参数 `use_pack_sequence`。 目前，`pack_padded_sequence` 函数要求长度序列张量在 CPU 设备上，因此训练函数需要避免在训练时将长度序列数据移动到 GPU。 您可以在位于本地目录的 `torchnlp.py` 文件中查看 `train_epoch_emb` 辅助函数的实现。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for label,text,off in test_loader_len:\n",
        "        \n",
        "        text, label = text.to(device), label.to(device)\n",
        "        off = off.to('cpu')\n",
        "        print(f'off value: {off}')\n",
        "        pred = net(text, off )\n",
        "        print(f'target {label}')\n",
        "        y=torch.argmax(pred, dim=1)\n",
        "        print(f'pred: {y}')\n",
        "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(y[0],classes[y[0]]))\n",
        "        print(\"Target:\\nvalue={}, class_name= {}\\n\".format(label[0],classes[label[0]]))\n",
        "        break\n",
        "     "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "off value: tensor([39, 54, 44, 49, 31, 34, 45, 29, 47, 48, 37, 39, 27, 37, 35, 47])\nlabel: tensor([2, 2, 2, 2, 0, 3, 1, 1, 2, 3, 3, 1, 3, 1, 2, 1])\npred: tensor([2, 2, 2, 2, 0, 3, 1, 1, 2, 3, 3, 1, 0, 1, 2, 1])\nPredicted:\nvalue=2, class_name= Business\n\nTarget:\nvalue=2, class_name= Business\n\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional and multilayer RNNs\n",
        "\n",
        "In our examples, all recurrent networks operated in one direction, from beginning of a sequence to the end. It looks natural, because it resembles the way we read and listen to speech. However, since in many practical cases we have random access to the input sequence, it might make sense to run recurrent computation in both directions. Such networks are call **bidirectional** RNNs, and they can be created by passing `bidirectional=True` parameter to RNN/LSTM/GRU constructor.\n",
        "\n",
        "> **Example**:   _self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True, bidrectional=True)_\n",
        "\n",
        "When dealing with bidirectional network, we would need two hidden state vectors, one for each direction. PyTorch encodes those vectors as one vector of twice larger size, which is quite convenient, because you would normally pass the resulting hidden state to fully-connected linear layer, and you would just need to take this increase in size into account when creating the layer.\n",
        "\n",
        "Recurrent network, one-directional or bidirectional, captures certain patterns within a sequence, and can store them into state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns, build from low-level patterns extracted by the first layer. This leads us to the notion of **multi-layer RNN**, which consists of two or more recurrent networks, where output of the previous layer is passed to the next layer as input.\n",
        "\n",
        "在我们的示例中，所有循环网络都在一个方向上运行，从序列的开始到结束。 它看起来很自然，因为它类似于我们阅读和听演讲的方式。 然而，由于在许多实际情况下我们可以随机访问输入序列，因此在两个方向上运行循环计算可能是有意义的。 这样的网络称为**双向** RNN，它们可以通过将`bidirectional=True`参数传递给 RNN/LSTM/GRU 构造函数来创建。\n",
        "\n",
        "> **例子**：_self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True, bidrectional=True)_\n",
        "\n",
        "在处理双向网络时，我们需要两个隐藏状态向量，每个方向一个。 PyTorch 将这些向量编码为一个两倍大小的向量，这非常方便，因为您通常会将生成的隐藏状态传递给完全连接的线性层，并且您只需要在创建层时考虑到这种大小的增加 .\n",
        "\n",
        "循环网络，单向或双向，捕获序列中的某些模式，并将它们存储到状态向量中或传递到输出中。 与卷积网络一样，我们可以在第一个循环层之上构建另一个循环层以捕获更高级别的模式，从第一层提取的低级模式构建。 这使我们想到了**多层 RNN** 的概念，它由两个或多个循环网络组成，其中前一层的输出作为输入传递到下一层。\n",
        "\n",
        "<img alt=\"Image showing a Multilayer long-short-term-memory- RNN\" src=\"images/5-recurrent-networks-3.jpg\" align=\"middle\" />\n",
        "\n",
        "*Picture from \"From a LSTM cell to a Multilayer LSTM Network with PyTorch\" article by Fernando López*\n",
        "\n",
        "PyTorch makes constructing such networks an easy task, because you just need to pass `num_layers` parameter to RNN/LSTM/GRU constructor to build several layers of recurrence automatically. This would also mean that the size of hidden/state vector would increase proportionally, and you would need to take this into account when handling the output of recurrent layers.\n",
        "\n",
        "*图片来自 Fernando López 的文章“From a LSTM cell to a Multilayer LSTM Network with PyTorch”*\n",
        "\n",
        "PyTorch 使构建此类网络成为一项简单的任务，因为您只需将 num_layers 参数传递给 RNN/LSTM/GRU 构造函数即可自动构建多个递归层。 这也意味着隐藏/状态向量的大小将按比例增加，并且在处理循环层的输出时需要考虑到这一点。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNs for other tasks\n",
        "\n",
        "In this unit, we have seen that RNNs can be used for sequence classification, but in fact, they can handle many more tasks, such as text generation, machine translation, and more. We will consider those tasks in the next unit.\n",
        "\n",
        "在本单元中，我们看到 RNN 可以用于序列分类，但实际上，它们可以处理更多的任务，例如文本生成、机器翻译等。 我们将在下一个单元中考虑这些任务。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "azureml_py38_PT_and_TF",
      "language": "python",
      "name": "conda-env-azureml_py38_PT_and_TF-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "azureml_py38_PT_and_TF",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}