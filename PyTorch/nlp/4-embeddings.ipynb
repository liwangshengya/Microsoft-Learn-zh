{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "In our previous example, we operated on high-dimensional bag-of-words vectors with the length of `vocab_size`, and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot representations. \n",
        "\n",
        "在我们之前的示例中，我们对长度为`vocab_size`的高维词袋向量进行操作，并且我们明确地将低维位置表示向量转换为稀疏的单热表示。\n",
        "\n",
        "<img alt=\"diagram show how high-dimensional vectors are converted to embedding vectors\" src=\"images/4-embedding-1.png\" align=\"middle\" />\n",
        "\n",
        "The goal of using word embeddings and reducing the dimensionality are:\n",
        "-  Finding the meaning of words based on their word approximation to other words.  This is done by taken two word vectors and analyzing how often the words in the vectors are used together.  The higher the frequency, the more you can find a correlation and relationship between the words.  \n",
        "- This process of training the word embedding to find word approximations between words in a given dimension is how we reduce the word representation to low-dimensions.\n",
        "-  Embedding vectors serve as numeric representations of words and are used as input to other machine learning network layers.\n",
        "-  The embedding vector becomes the stored lookup table for words in the vocabulary\n",
        "\n",
        "In this unit, we will continue exploring the **News AG** dataset. To begin, let's load the data and get some definitions from the previous unit.  In addition, we will allocation our training and testing datasets; word vocabulary size; and the category of our word classes: _World_, _Sports_, _Business_ and _Sci/Tech_\n",
        "\n",
        "使用词嵌入和降维的目标是：\n",
        "- 根据单词与其他单词的近似值找出单词的含义。 这是通过获取两个词向量并分析向量中的词一起使用的频率来完成的。 频率越高，你就越能找到单词之间的关联和关系。\n",
        "- 这种训练词嵌入以在给定维度中找到词之间的词近似值的过程是我们如何将词表示减少到低维的。\n",
        "- 嵌入向量用作单词的数字表示，并用作其他机器学习网络层的输入。\n",
        "- 嵌入向量成为词汇表中单词的存储查找表\n",
        "\n",
        "在本单元中，我们将继续探索 **News AG** 数据集。 首先，让我们加载数据并从上一个单元获取一些定义。 此外，我们将分配我们的训练和测试数据集； 单词词汇量； 以及我们词类的类别：_World_、_Sports_、_Business_ 和 _Sci/Tech_"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "import numpy as np\n",
        "from torchnlp import *\n",
        "from torchinfo import summary\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset...\nBuilding vocab...\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with variable sequence size\n",
        "\n",
        "When working with words, you are going to have text sequences or sentences that are of different lengths.  This can be problematic in training the word embeddings neural network. For consistency in the word embedding and improve training performance, we would have to apply some padding. This can be done using the `torch.nn.functional.pad` on a tokenized dataset. It adds zero values to the empty indices at the end of the vector.\n",
        "\n",
        "使用单词时，您将拥有不同长度的文本序列或句子。 这在训练词嵌入神经网络时可能会出现问题。 为了词嵌入的一致性和提高训练性能，我们必须应用一些填充。 这可以在标记化数据集上使用 `torch.nn.functional.pad` 来完成。 它将零值添加到向量末尾的空索引。\n",
        "\n",
        "<img alt=\"diagram show how showing padding\" src=\"images/4-embedding-2.png\" align=\"middle\" />\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def padify(b):\n",
        "    # b is the list of tuples of length batch_size\n",
        "    #   - first element of a tuple = label, \n",
        "    #   - second = feature (text sequence)\n",
        "    # build vectorized sequence\n",
        "    v = [encode(x[1]) for x in b]\n",
        "    # first, compute max length of a sequence in this minibatch\n",
        "    l = max(map(len,v))\n",
        "    return ( # tuple of two tensors - labels and features\n",
        "        torch.LongTensor([t[0]-1 for t in b]),\n",
        "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the first 2 sentences as example to view the text length differences and effects of padding.\r\n",
        "\r\n",
        "我们以前两句为例，看看文本长度的差异和padding的效果。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = train_dataset[0][1]\n",
        "second_sentence = train_dataset[1][1]\n",
        "\n",
        "f_tokens = encode(first_sentence)\n",
        "s_tokens = encode(second_sentence)\n",
        "\n",
        "print(f'First Sentence in dataset:\\n{first_sentence}')\n",
        "print(\"Length:\", len(train_dataset[0][1]))\n",
        "print(f'\\nSecond Sentence in dataset:\\n{second_sentence}')\n",
        "print(\"Length: \", len(train_dataset[1][1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "First Sentence in dataset:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\nLength: 144\n\nSecond Sentence in dataset:\nCarlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\nLength:  266\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the text sequence from the news article headlines in our dataset to change into a tokenize vector.  As you will see, the text sequence have different lengths.  We'll apply padding so all the text sequence will have a fixed length.  This approach is used when you have a large set of text sequences in your dataset.\n",
        "\n",
        "- The length of the 1st and 2nd sentences displayed have difference lengths.  \n",
        "- The max length of the dataset tensors is the length of the longest sentence length in the entire dataset.\n",
        "- The zeros are added to the empty indexes in the tensor.\n",
        "\n",
        "让我们使用数据集中新闻文章标题中的文本序列转换为标记化向量。 正如您将看到的，文本序列有不同的长度。 我们将应用填充，以便所有文本序列都具有固定长度。 当您的数据集中有大量文本序列时，可以使用此方法。\n",
        "\n",
        "- 显示的第 1 句和第 2 句的长度不同。\n",
        "- 数据集张量的最大长度是整个数据集中最长句子长度的长度。\n",
        "- 零被添加到张量中的空索引。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "labels, features = padify(train_dataset)  \n",
        "print(f'features: {features}')\n",
        "\n",
        "print(f'\\nlength of first sentence: {len(f_tokens)}')\n",
        "print(f'length of second sentence: {len(s_tokens)}')\n",
        "print(f'size of features: {features.size()}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "features: tensor([[  432,   426,     2,  ...,     0,     0,     0],\n        [15875,  1073,   855,  ...,     0,     0,     0],\n        [   59,     9,   348,  ...,     0,     0,     0],\n        ...,\n        [ 7736,    63,   665,  ...,     0,     0,     0],\n        [   97,    17,    10,  ...,     0,     0,     0],\n        [ 2155,   223,  2405,  ...,     0,     0,     0]])\n\nlength of first sentence: 29\nlength of second sentence: 42\nsize of features: torch.Size([120000, 207])\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What is embedding?\n",
        "\n",
        "The idea of **embedding** is the process of mapping words into vectors, which reflects the **_semantic meaning of a word_**. The length of its vectors are the embedding dimensions size. We will later discuss how to build meaningful word embeddings, but for now let's just think of embeddings as a way to lower dimensionality of a word vector. \n",
        "\n",
        "So, embedding layer would take a word as an input, and produce an output vector of specified `embedding_size`. In a sense, it is very similar to `Linear` layer, but instead of taking one-hot encoded vector, it will be able to take a word number as an input.\n",
        "\n",
        "By using embedding layer as a first layer in our network, we can switch from bag-or-words to **embedding bag** model, where we first convert each word in our text into corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `average` or `max`.  \n",
        "\n",
        "**embedding**的思想是将词映射成向量的过程，反映了词的**_语义_**。 其向量的长度是嵌入维度大小。 我们稍后将讨论如何构建有意义的词嵌入，但现在让我们将嵌入视为降低词向量维数的一种方式。\n",
        "\n",
        "因此，嵌入层会将一个词作为输入，并产生指定`embedding_size`的输出向量。 从某种意义上说，它与`Linear`层非常相似，但它不是采用单热编码向量，而是能够采用单词编号作为输入。\n",
        "\n",
        "通过使用嵌入层作为我们网络中的第一层，我们可以从 bag-or-words 切换到 **embedding bag** 模型，我们首先将文本中的每个单词转换为相应的嵌入，然后计算一些聚合函数 所有这些嵌入，例如 `sum`、`average` 或 `max`。\n",
        "\n",
        "<img alt=\"Image showing an embedding classifier for five sequence words.\" src=\"images/4-embedding-3.png\" align=\"middle\" />\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our classifier neural network will start with an embedding layer, then aggregation layer, and a linear classifier on top of it:\n",
        "- `vocab_size` are the size of the total number of words we have in our vocabulary.\n",
        "- `embed_dim` are the length of the word dimensions that show relationships between words passed as in the network.\n",
        "- `num_class` are the number of news categories we are trying to classify (e.g. World, Sports, Business, Sci/Tech) \n",
        "\n",
        "我们的分类器神经网络将从嵌入层开始，然后是聚合层，然后是其上的线性分类器：\n",
        "- `vocab_size` 是我们词汇表中单词总数的大小。\n",
        "- `embed_dim` 是单词维度的长度，显示网络中传递的单词之间的关系。\n",
        "- `num_class` 是我们要分类的新闻类别的数量（例如世界、体育、商业、科技）\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x,dim=1)\n",
        "        return self.fc(x)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training embedding classifier\n",
        "\n",
        "Now we’ll define our training dataloader and use the `collate_fn` to apply the padify function to the datasets as they loaded in each batch.  As a result, the training dataset will be padded.\n",
        "\n",
        "现在我们将定义我们的训练数据加载器，并使用 `collate_fn` 将 padify 函数应用于每批加载的数据集。 结果，训练数据集将被填充。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can train the model using the training function defined in the previous unit to run the embedding network.  The training output serves as a vector lookup store based on the unique index tokens from the vocabulary.\r\n",
        "\r\n",
        "我们可以使用上一单元中定义的训练函数来训练模型来运行嵌入网络。 训练输出用作基于词汇表中唯一索引标记的向量查找存储。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.64875\n6400: acc=0.69234375\n9600: acc=0.7110416666666667\n12800: acc=0.72421875\n16000: acc=0.73625\n19200: acc=0.7476041666666666\n22400: acc=0.7541964285714285\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "(0.9072840441058861, 0.7583173384516955)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: We are only training for 25k records here (less than one full epoch) for the sake of time, but you can continue training, write a function to train for several epochs, and experiment with learning rate parameter to achieve higher accuracy. You should be able to go to the accuracy of about 90%.\r\n",
        "\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **注意**：为了节省时间，我们这里只训练 25k 记录（少于一个完整的 epoch），但是你可以继续训练，写一个函数来训练几个 epoch，并用学习率参数进行实验 达到更高的准确度。 你应该可以达到 90% 左右的准确率。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmbeddingBag Layer and Variable-Length Sequence Representation\n",
        "\n",
        "In the previous architecture, we needed to pad all sequences to the same length in order to fit them into a minibatch. This is not the most efficient way to represent variable length sequences - another apporach would be to use **offset** vector, which would hold offsets of all sequences stored in one large vector.\n",
        "\n",
        "在之前的架构中，我们需要将所有序列填充到相同的长度，以便将它们放入一个小批量中。 这不是表示可变长度序列的最有效方法 - 另一种方法是使用 **offset** 向量，它将保存存储在一个大向量中的所有序列的偏移量。\n",
        "\n",
        "<img alt=\"Image showing an offset sequence representation\" src=\"images/4-embedding-4.png\" align=\"middle\" />\n",
        "\n",
        "> **Note**: On the picture above, we show a sequence of characters, but in our example we are working with sequences of words. However, the general principle of representing sequences with offset vector remains the same.\n",
        "\n",
        "To work with offset representation, we use PyTorch's `EmbeddingBag` layer. It is similar to `Embedding`, but it takes content vector and offset vector as input, and it also includes averaging layer, which can be `mean`, `sum` or `max`.\n",
        "\n",
        "Here is modified network that uses `EmbeddingBag`:\n",
        "\n",
        "> **注意**：在上图中，我们展示了一个字符序列，但在我们的示例中，我们使用的是单词序列。 然而，用偏移向量表示序列的一般原则保持不变。\n",
        "\n",
        "为了使用偏移表示，我们使用 PyTorch 的“EmbeddingBag”层。 它类似于`Embedding`，但它以内容向量和偏移向量作为输入，它还包括平均层，可以是`mean`、`sum` 或`max`。\n",
        "\n",
        "这是使用 `EmbeddingBag` 的修改后的网络：\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, text, off):\n",
        "        x = self.embedding(text, off)\n",
        "        return self.fc(x)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the dataset for training, we need to provide a conversion function that will prepare the offset vector:\r\n",
        "\r\n",
        "为了准备训练数据集，我们需要提供一个转换函数来准备偏移向量："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1])) for t in b]\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The offset vector is calculated by first combining the sentences indices into one tensor sequence, then extracting the starting index location of each sentence in the sequence. For example: \n",
        "- The length of the first sentence in our training dataset is 29.  Meaning the first index of the offset will be `0`.\n",
        "- The length of the second sentence in the dataset is 42.  Meaning the second index of the offset will be `29`, where the first sentence ended. \n",
        "- The third index of the offset will be 29 + 42 = `71`, where the 2nd sentence ended.\n",
        "\n",
        "偏移向量的计算方法是首先将句子索引组合成一个张量序列，然后提取序列中每个句子的起始索引位置。 例如：\n",
        "- 我们训练数据集中第一个句子的长度是 29。这意味着偏移量的第一个索引将为`0`。\n",
        "- 数据集中第二个句子的长度是 42。这意味着偏移量的第二个索引将是`29`，第一个句子结束的地方。\n",
        "- 偏移量的第三个索引将是 29 + 42 = `71`，第二句结束的地方。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "labels, features, offset = offsetify(train_dataset)  \n",
        "print(f'offset: {offset}')\n",
        "print(f'\\nlength of first sentence: {len(f_tokens)}')\n",
        "print(f'length of second sentence: {len(s_tokens)}')\n",
        "print(f'size of data vector: {features.size()}')\n",
        "print(f'size of offset vector: {offset.size()}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "offset: tensor([      0,      29,      71,  ..., 5193441, 5193488, 5193569])\n\nlength of first sentence: 29\nlength of second sentence: 42\nsize of data vector: torch.Size([5193609])\nsize of offset vector: torch.Size([120000])\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** that unlike in all previous examples, our network now accepts two parameters: data vector and offset vector, which are of different sizes. Similarly, our data loader also provides us with 3 values instead of 2: both text and offset vectors are provided as features. Therefore, we need to slightly adjust our training function to take care of that:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **注意：**与之前的所有示例不同，我们的网络现在接受两个参数：数据向量和偏移向量，它们具有不同的大小。 同样，我们的数据加载器也为我们提供了 3 个值而不是 2 个：文本和偏移向量都作为特征提供。 因此，我们需要稍微调整我们的训练函数来解决这个问题："
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "\n",
        "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    loss_fn = loss_fn.to(device)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,text,off in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
        "        out = net(text, off)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count\n",
        "\n",
        "\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.6496875\n6400: acc=0.6853125\n9600: acc=0.7097916666666667\n12800: acc=0.725546875\n16000: acc=0.737375\n19200: acc=0.7446354166666667\n22400: acc=0.7532589285714286\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "(22.52296015275112, 0.7591570697376839)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Semantic Embeddings: Word2Vec\n",
        "\n",
        "In our previous example, the model embedding layer learnt to map words to vector representation, however, this representation did not have much semantical meaning. It would be nice to learn such vector representation, that similar words or symonims would correspond to vectors that are close to each other in terms of some vector distance (eg. euclidian distance).\n",
        "\n",
        "To do that, we need to pre-train our embedding model on a large collection of text in a specific way. One of the first ways to train semantic embeddings is called **Word2Vec**.   It helps **_map the probability of a word_**, based on the contexts from texts in the sequence.  It is based on two main architectures that are used to produce a distributed representation of words:\n",
        "\n",
        " - **Continuous bag-of-words** (CBoW) — in this architecture, we train the model to predict a word from surrounding context. Given the ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, the goal of the model is to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.  For example:  **_\"I like my hot dog on a __\"_**.  Here the predicted word would be **_\"bun\"_**.\n",
        " - **Continuous skip-gram** is opposite to CBoW. The model uses surrounding window of context words to predict the current word.  For example: you can predict **_dog_** to be more associated with the word **_veterinary_**.\n",
        "\n",
        "CBoW is faster, while skip-gram is slower, but does a better job of representing infrequent words.\n",
        "\n",
        "在我们之前的例子中，模型嵌入层学会了将单词映射到向量表示，然而，这种表示没有太多的语义意义。 学习这样的向量表示会很好，相似的词或符号对应于在某种向量距离（例如欧几里德距离）方面彼此接近的向量。\n",
        "\n",
        "为此，我们需要以特定方式在大量文本上预训练我们的嵌入模型。 训练语义嵌入的第一种方法称为 **Word2Vec**。 它有助于根据序列中文本的上下文**_映射一个词的概率_**。 它基于两个主要的架构，用于生成单词的分布式表示：\n",
        "\n",
        "  - **连续词袋**（CBoW）——在这个架构中，我们训练模型从周围的上下文中预测一个词。 给定 ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$，模型的目标是从 $(W_{-2},W_{-1} ,W_1,W_2)$。 例如：**_“我喜欢 __ 上的热狗”_**。 这里预测的词是 **_\"bun\"_**。\n",
        "  - **Continuous skip-gram** 与 CBoW 相反。 该模型使用上下文词的周围窗口来预测当前词。 例如：您可以预测 **_dog_** 与 **_veterinary_** 这个词的关联度更高。\n",
        "\n",
        "CBoW 更快，而 skip-gram 更慢，但在表示不常用词方面做得更好。\n",
        "\n",
        "<img alt=\"Image showing both CBoW and Skip-Gram algorithms to convert words to vectors.\" src=\"images/4-embedding-5.png\" align=\"middle\" />\n",
        "\n",
        "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context.   \n",
        "\n",
        "- **GloVe** (Global Vectors) - derives the relationship between words. It determines how often a particular word-pair occurs together in a text by leveraging the idea of co-occurence matrix and uses neural methods to decompose co-occurrence matrix into more expressive and non linear word vectors.\n",
        "- **FastText** - builds on Word2Vec by learning vector representations for each word and the charachter n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pre-training, it enables word embeddings to encode sub-word information.\n",
        "- **Gensim** (Generate Similar) - is an open source NLP Python library that provides a unified interface to build word vectors, corpus, perform topic identification, and other NLP tasks.\n",
        "\n",
        "FastText and GloVe are other word embeddings techniques that predict the probably of words appearing together.  \n",
        "\n",
        "In our Word2Vec examples, we'll using pre-trained semantic embeddings, but it is interesting to see how those embeddings can be trained using either FastText, CBoW, or Skip-gram architectures. This exercise goes beyond this module, but those interested can reference Word Embeddings tutorials on Pytorch's website. \n",
        "\n",
        "CBOW 和 Skip-Grams 都是“预测”嵌入，因为它们只考虑局部上下文。 Word2Vec 不利用全局上下文。\n",
        "\n",
        "- **GloVe** (Global Vectors) - 导出单词之间的关系。 它通过利用共现矩阵的思想确定特定词对在文本中一起出现的频率，并使用神经方法将共现矩阵分解为更具表现力和非线性的词向量。\n",
        "- **FastText** - 通过学习每个单词的向量表示和在每个单词中找到的字符 n-gram，以 Word2Vec 为基础。 然后在每个训练步骤将表示的值平均到一个向量中。 虽然这为预训练增加了大量额外的计算，但它使词嵌入能够对子词信息进行编码。\n",
        "- **Gensim** (Generate Similar) - 是一个开源的 NLP Python 库，它提供了一个统一的接口来构建词向量、语料库、执行主题识别和其他 NLP 任务。\n",
        "\n",
        "FastText 和 GloVe 是其他词嵌入技术，可以预测词同时出现的可能性。\n",
        "\n",
        "在我们的 Word2Vec 示例中，我们将使用预训练的语义嵌入，但有趣的是看看如何使用 FastText、CBoW 或 Skip-gram 架构来训练这些嵌入。 本练习超出了本模块的范围，但感兴趣的人可以参考 Pytorch 网站上的 Word Embeddings 教程。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genim\n",
        "\n",
        "The **gensim** framework can be used with Pytorch to train most commonly used embeddings in a few lines of code.  To experiment with word2vec embedding pre-trained on Google News dataset, we can use the **gensim** library. Below we find the words that are most similar to 'neural'\n",
        "\n",
        "> **Note:** When you first create word vectors, downloading them can take some time!\n",
        "\n",
        "**gensim** 框架可以与 Pytorch 一起使用，只需几行代码即可训练最常用的嵌入。 为了试验在 Google 新闻数据集上预训练的 word2vec 嵌入，我们可以使用 **gensim** 库。 下面我们找到与“neural”最相似的词\n",
        "\n",
        "> **注意：** 当您第一次创建词向量时，下载它们可能需要一些时间！"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at words that are similar to 'dog'."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('dog'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "dogs -> 0.8680489659309387\npuppy -> 0.8106428384780884\npit_bull -> 0.780396044254303\npooch -> 0.7627377510070801\ncat -> 0.7609456777572632\ngolden_retriever -> 0.7500902414321899\nGerman_shepherd -> 0.7465174198150635\nRottweiler -> 0.7437614798545837\nbeagle -> 0.7418621778488159\npup -> 0.740691065788269\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "We can also extract vector embeddings from the word, to be used in training classification model (we'll only show first 20 components of the vector for clarity):\r\n",
        "\r\n",
        "我们还可以从单词中提取向量嵌入，用于训练分类模型（为清楚起见，我们只显示向量的前 20 个分量）："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.word_vec('play')[:20]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n      dtype=float32)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The great thing about semantical embeddings is that you can manipulate vector encoding to change the semantics. For example, we can ask to find a word, whose vector representation would be as close as possible to words *king* and *woman*, and as far away from the word *man*:\r\n",
        "\r\n",
        "语义嵌入的伟大之处在于您可以操纵向量编码来改变语义。 例如，我们可以要求找到一个词，其向量表示尽可能接近词 *king* 和 *woman*，并尽可能远离词 *man*："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "('queen', 0.7118192911148071)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pre-Trained Embeddings in PyTorch\n",
        "\n",
        "We can modify the example above to pre-populate the matrix in our embedding layer with semantical embeddings, such as Word2Vec. We need to take into account that vocabularies of pre-trained embedding are an addition to the existing text corpus that we already have so they will likely not match. As a result, we will initialize weights for the missing words with random values:\n",
        "\n",
        "我们可以修改上面的示例，以使用语义嵌入（例如 Word2Vec）在我们的嵌入层中预填充矩阵。 我们需要考虑到预训练嵌入的词汇表是对我们已有的现有文本语料库的补充，因此它们可能不匹配。 因此，我们将使用随机值初始化缺失词的权重："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
        "\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab.itos):\n",
        "    try:\n",
        "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
        "        found+=1\n",
        "    except:\n",
        "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")\n",
        "net = net.to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Embedding size: 300\nPopulating matrix, this will take some time...Done, found 41080 words, 54732 words missing\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train our model. Note that the time it takes to train the model is significantly larger than in the previous example, due to larger embedding layer size, and thus much higher number of parameters. Also, because of this, we may need to train our model on more examples if we want to avoid overfitting.\r\n",
        "\r\n",
        "现在让我们训练我们的模型。 请注意，由于更大的嵌入层大小，因此训练模型所需的时间明显大于前面的示例，因此参数数量也更多。 此外，正因为如此，如果我们想避免过度拟合，我们可能需要在更多示例上训练我们的模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.6409375\n6400: acc=0.6875\n9600: acc=0.7163541666666666\n12800: acc=0.730859375\n16000: acc=0.740875\n19200: acc=0.7510416666666667\n22400: acc=0.7584821428571429\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "(228.52019353806782, 0.7641154830454254)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case we do not see a huge increase in accuracy, which is likely due to the quite different vocalularies. \n",
        "To overcome the problem of different vocabularies, we can use one of the following solutions:\n",
        "* Re-train word2vec model on our vocabulary\n",
        "* Load our dataset with the vocabulary from the pre-trained word2vec model. Vocabulary used to load the dataset can be specified during loading.\n",
        "\n",
        "The latter approach seems easier, especially because PyTorch `torchtext` framework contains built-in support for embeddings. \n",
        "\n",
        "在我们的例子中，我们没有看到准确性的巨大提高，这可能是由于完全不同的发音。\n",
        "为了克服不同词汇表的问题，我们可以使用以下解决方案之一：\n",
        "* 在我们的词汇表上重新训练 word2vec 模型\n",
        "* 使用预训练的 word2vec 模型中的词汇加载我们的数据集。 用于加载数据集的词汇可以在加载期间指定。\n",
        "\n",
        "后一种方法似乎更容易，特别是因为 PyTorch `torchtext` 框架包含对嵌入的内置支持。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVe Embeddings\n",
        "\n",
        "To load our dataset with the vocabulary from a pre-trained word2vec model, we use Glove embeddings. We'll start by instantiating GloVe-based vocabulary in the following manner:\n",
        "\n",
        "为了使用来自预训练的 word2vec 模型的词汇加载我们的数据集，我们使用 Glove 嵌入。 我们将从以下方式实例化基于 GloVe 的词汇表开始："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loaded vocabulary has the following basic operations:\n",
        "* `vocab.stoi` dictionary allows us to convert word into its dictionary index\n",
        "* `vocab.itos` does the opposite - converts number into word\n",
        "* `vocab.vectors` is the array of embedding vectors, so to get the embedding of a word we need to use `vocab.vectors[vocab.stoi[s]]`\n",
        "\n",
        "Here is the example of manipulating embeddings to demonstrate the equation **kind-man+woman = queen** (the coefficient was tweak a bit to make it work):\n",
        "\n",
        "加载词汇表有以下基本操作：\n",
        "* `vocab.stoi` 字典允许我们将单词转换成它的字典索引\n",
        "* `vocab.itos` 做相反的事情——将数字转换成单词\n",
        "* `vocab.vectors` 是嵌入向量的数组，因此要获得单词的嵌入，我们需要使用 `vocab.vectors[vocab.stoi[s]]`\n",
        "\n",
        "下面是操纵嵌入来演示等式 **kind-man+woman = queen** 的示例（对系数进行了一些调整以使其起作用）："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
        "# find the index of the closest embedding vector \n",
        "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
        "min_idx = torch.argmin(d)\n",
        "# find the corresponding word\n",
        "vocab.itos[min_idx]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "'queen'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the classifier using those embeddings, we first need to encode our dataset using GloVe vocabulary:\r\n",
        "\r\n",
        "要使用这些嵌入训练分类器，我们首先需要使用 GloVe 词汇表对数据集进行编码："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen above, all vector embeddings are stored in `vocab.vectors` matrix. It makes it super-easy to load those weights into weights of embedding layer using simple copying:\r\n",
        "\r\n",
        "正如我们在上面看到的，所有向量嵌入都存储在`vocab.vectors`矩阵中。 使用简单的复制就可以非常容易地将这些权重加载到嵌入层的权重中："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
        "net.embedding.weight.data = vocab.vectors\n",
        "net = net.to(device)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "Now let's train our model and see if we get better results:\r\n",
        "\r\n",
        "现在让我们训练我们的模型，看看我们是否能得到更好的结果："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.6359375\n6400: acc=0.68203125\n9600: acc=0.706875\n12800: acc=0.727734375\n16000: acc=0.738625\n19200: acc=0.7465104166666666\n22400: acc=0.7526785714285714\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "(35.71297184900832, 0.7573576455534229)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the reasons we are not seeing a significant increase in accuracy is due to the fact that some words from our dataset are missing in the pre-trained GloVe vocabulary, and thus they are essentially ignored. To overcome this fact, we can train our own embeddings on our dataset.\r\n",
        "\r\n",
        "我们没有看到准确性显着提高的原因之一是我们数据集中的一些词在预训练的 GloVe 词汇表中缺失，因此它们基本上被忽略了。 为了克服这个事实，我们可以在我们的数据集上训练我们自己的嵌入。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual Embeddings\n",
        "\n",
        "One key limitation of traditional pretrained embedding representations such as Word2Vec is the problem of word sense and removing ambiguity by making them clear. While pretrained embeddings can capture some of the meaning of words in context, every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models, since many words such as the word 'play' have different meanings depending on the context they are used in.\n",
        "\n",
        "For example, the word 'play' in these two different sentences have quite different meaning:\n",
        "- I went to a **play** at the theatre.\n",
        "- John wants to **play** with his friends.\n",
        "\n",
        "The pretrained embeddings above represent both meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the **language model**, which is trained on a large corpus of text, and *knows* how words can be put together in different contexts. Discussing contextual embeddings is out of scope for this tutorial, but we will come back to them when talking about language models in the next unit.\n",
        "\n",
        "传统预训练嵌入表示（如 Word2Vec）的一个关键限制是词义问题和通过使它们清晰来消除歧义。 虽然预训练嵌入可以捕获上下文中单词的某些含义，但单词的每个可能含义都被编码到相同的嵌入中。 这可能会导致下游模型出现问题，因为许多词（例如“play”）根据其使用的上下文具有不同的含义。\n",
        "\n",
        "例如，这两个不同句子中的“play”一词具有完全不同的含义：\n",
        "- 我去剧院看了一场**戏**。\n",
        "- 约翰想和他的朋友们**玩**。\n",
        "\n",
        "上面的预训练嵌入表示同一嵌入中单词“play”的两种含义。 为了克服这一限制，我们需要基于**语言模型**构建嵌入，该模型在大量文本语料库上进行训练，并且*knows*如何将单词放在不同的上下文中。 讨论上下文嵌入超出了本教程的范围，但我们将在下一个单元中讨论语言模型时回到它们。\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-azureml_py38_PT_and_TF-py",
      "language": "python",
      "display_name": "azureml_py38_PT_and_TF"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-azureml_py38_PT_and_TF-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}