{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of the training process requires calculating derivatives that involve tensors. So let's learn about TensorFlow's built-in [automatic differentiation](https://www.tensorflow.org/guide/autodiff) engine, using a very simple example. Let's consider the following two tensors:\n",
        "\n",
        "部分训练过程需要计算涉及张量的导数。 那么让我们通过一个非常简单的例子来了解一下TensorFlow内置的[自动微分](https://www.tensorflow.org/guide/autodiff)引擎。 让我们考虑以下两个张量：\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    1 & 2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    3 & 4 \\\\\n",
        "    5 & 6\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now let's suppose that we want to multiply $U$ by $V$, and then sum all the values in the resulting tensor, such that the result is a scalar. In math notation, we might represent this as the following scalar function $f$:\n",
        "\n",
        "现在假设我们要将 $U$ 乘以 $V$，然后对生成的张量中的所有值求和，这样结果就是一个标量。 在数学符号中，我们可以将其表示为以下标量函数 $f$：\n",
        "\n",
        "$$\n",
        "f(U, V) = \\mathrm{sum} (U \\, V) = \\sum_j \\sum_i u_i \\, v_{ij}\n",
        "$$\n",
        "\n",
        "Our goal is to calculate the derivative of $f$ with respect to each of its inputs: $\\frac{\\partial f}{\\partial U}$ and $\\frac{\\partial f}{\\partial V}$. We start by creating the two tensors $U$ and $V$. We then create a [tf.GradientTape](https://www.tensorflow.org/guide/autodiff#gradient_tapes), and tell TensorFlow to watch for mathematical operations involving $U$ and $V$, recording those operations onto our \"tape.\" The tape then enables us to calculate the derivatives of the function $f$ with respect to $U$ and $V$.\n",
        "\n",
        "我们的目标是计算 $f$ 对其每个输入的导数：$\\frac{\\partial f}{\\partial U}$ 和 $\\frac{\\partial f}{\\partial V}$。 我们首先创建两个张量 $U$ 和 $V$。 然后我们创建一个 [tf.GradientTape](https://www.tensorflow.org/guide/autodiff#gradient_tapes)，并告诉 TensorFlow 观察涉及 $U$ 和 $V$ 的数学运算，将这些运算记录到我们的`tape`. 然后磁带使我们能够计算函数 $f$ 关于 $U$ 和 $V$ 的导数。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.constant([[1., 2.]])\n",
        "V = tf.constant([[3., 4.], [5., 6.]])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(U)\n",
        "  tape.watch(V)\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[ 7. 11.]], shape=(1, 2), dtype=float32)\ntf.Tensor(\n[[1. 1.]\n [2. 2.]], shape=(2, 2), dtype=float32)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow automatically watches tensors that are defined as `Variable` instances. So let's turn `U` and `V` into variables, and remove the `watch` calls:\r\n",
        "\r\n",
        "TensorFlow 自动监视定义为`Variable`实例的张量。 因此，让我们将 `U` 和 `V` 变成变量，并删除 `watch` 调用："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.Variable(tf.constant([[1., 2.]]))\n",
        "V = tf.Variable(tf.constant([[3., 4.], [5., 6.]]))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[ 7. 11.]], shape=(1, 2), dtype=float32)\ntf.Tensor(\n[[1. 1.]\n [2. 2.]], shape=(2, 2), dtype=float32)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you will see later, in deep learning, we will need to calculate the derivatives of the loss function with respect to the model parameters. Those parameters are variables because they change during training. Therefore, the fact that variables are automatically watched is handy in our scenario.  \r\n",
        "\r\n",
        "后面你会看到，在深度学习中，我们需要计算损失函数对模型参数的导数。 这些参数是变量，因为它们在训练期间会发生变化。 因此，自动监视变量这一事实在我们的场景中很方便。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional explanation of the math\n",
        "\n",
        "Let's take a look at the math used to compute the derivatives. You only need to understand matrix multiplication and partial derivatives to follow along, but if the math isn't as interesting to you, feel free to skip to the next notebook.\n",
        "\n",
        "We'll start by thinking of $U$ and $V$ as generic 1 &times; 2 and 2 &times; 2 matrices:\n",
        "\n",
        "让我们来看看用于计算导数的数学知识。你只需要了解矩阵乘法和部分导数就可以跟上，但如果你对数学不感兴趣，可以跳到下一个笔记本。\n",
        "\n",
        "我们先把$U$和$V$看作是一般的1 &times; 2和2 &times; 2矩阵。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    v_{11} & v_{12} \\\\\n",
        "    v_{21} & v_{22}\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Then the scalar function $f$ can be written as:\n",
        "\n",
        "那么标量函数$f$可以写成：\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  f(U, V)\n",
        "  &= \\mathrm{sum}(U \\, V) \\\\\n",
        "  &= \\mathrm{sum} \n",
        "    \\left( \n",
        "      \\begin{bmatrix}\n",
        "        u_1 & u_2\n",
        "      \\end{bmatrix}\n",
        "      \\begin{bmatrix}\n",
        "        v_{11} & v_{12} \\\\\n",
        "        v_{21} & v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= \\mathrm{sum}\n",
        "    \\left(\n",
        "      \\begin{bmatrix}\n",
        "        u_1 v_{11} + u_2 v_{21} & u_1 v_{12} + u_2 v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= u_1 v_{11} + u_2 v_{21} + u_1 v_{12} + u_2 v_{22}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "We can now calculate the derivatives of $f$ with respect to each of its inputs:\n",
        "\n",
        "我们现在可以计算 $f$ 对每个输入的导数：\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial U} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial u_1} & \\frac{\\partial f}{\\partial u_2}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    v_{11} + v_{12} & v_{21} + v_{22}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    7 & 11\n",
        "  \\end{bmatrix} \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial V} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial v_{11}} & \\frac{\\partial f}{\\partial v_{12}} \\\\\n",
        "    \\frac{\\partial f}{\\partial v_{21}} & \\frac{\\partial f}{\\partial v_{22}} \n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_1 \\\\\n",
        "    u_2 & u_2\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    2 & 2\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "As you can see, when we plug in the numerical values of $U$ and $V$, we get the same result as TensorFlow's automatic differentiation.\n",
        "\n",
        "\n",
        "如您所见，当我们插入 $U$ 和 $V$ 的数值时，我们得到与 TensorFlow 的自动微分相同的结果。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "azureml_py38"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}