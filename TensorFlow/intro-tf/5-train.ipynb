{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import gzip\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've already seen how to train a neural network using Keras in [module 1](../intro-machine-learning-keras) &mdash; in this notebook, we'll re-implement the training loop in TensorFlow. This will help you understand what goes on under the hood a bit better, will give you the opportunity to customize the training loop if you want, and will enable you to debug it.\n",
        "\n",
        "We'll start by including code that gives us the datasets and model that we'll use in the remainder of this notebook. We will use the same FashionMNIST dataset and data loading code as in [module 1](../intro-machine-learning-keras), so feel free to re-visit that module if something is not clear, or take a look [at the source code](https://github.com/MicrosoftDocs/tensorflow-learning-path/blob/main/intro-tf/tintro.py).\n",
        "\n",
        "您已经在 [模块 1](../intro-machine-learning-keras) 中了解了如何使用 Keras 训练神经网络 &mdash; 在本笔记本中，我们将在 TensorFlow 中重新实现训练循环。 这将帮助您更好地了解引擎盖下发生的事情，让您有机会根据需要自定义训练循环，并使您能够对其进行调试。\n",
        "\n",
        "我们将首先包括为我们提供数据集和模型的代码，我们将在本笔记本的其余部分中使用这些代码。 我们将使用与 [模块 1](../intro-machine-learning-keras) 中相同的 FashionMNIST 数据集和数据加载代码，所以如果有什么不清楚的地方，请随时重新访问该模块，或者看看 [ 在源代码](https://github.com/MicrosoftDocs/tensorflow-learning-path/blob/main/intro-tf/tintro.py)。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-tf/tintro.py\n",
        "from tintro import *"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned in module 1, the goal of training the neural network is to find parameters $W$ and $b$ that minimize the loss function, which measures the difference between the actual and predicted labels. We also mentioned that we can think of the neural network as the function $\\ell$ below, and that we use an optimization algorithm to find the parameters $W$ and $b$ that minimize this function.\r\n",
        "\r\n",
        "正如我们在模块 1 中提到的，训练神经网络的目标是找到使损失函数最小化的参数 $W$ 和 $b$，损失函数衡量实际标签和预测标签之间的差异。 我们还提到，我们可以将神经网络视为下面的函数 $\\ell$，并且我们使用优化算法来找到使该函数最小化的参数 $W$ 和 $b$。\r\n",
        "\r\n",
        "$$\r\n",
        "\\mathrm{loss} = \\ell(X, y, W, b)\r\n",
        "$$\r\n",
        "\r\n",
        "Let's now dig deeper into what this optimization algorithm might look like. There are many types of optimization algorithms, but in this tutorial we'll cover only the simplest one: the gradient descent algorithm. To implement gradient descent, we iteratively improve our estimates of $W$ and $b$ according to the update formulas below, until the gradients are smaller than a pre-defined threshold $\\epsilon$ (or for a pre-defined number of times):\r\n",
        "\r\n",
        "现在让我们更深入地研究一下这个优化算法可能是什么样子。 优化算法有很多种，但在本教程中我们只介绍最简单的一种：梯度下降算法。 为了实现梯度下降，我们根据下面的更新公式迭代地改进我们对 $W$ 和 $b$ 的估计，直到梯度小于预定义的阈值 $\\epsilon$（或预定义的次数 ):\r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{align}\r\n",
        "  W &:= W - \\alpha \\frac{\\partial \\ell}{\\partial W} \\\\\r\n",
        "  b &:= b - \\alpha \\frac{\\partial \\ell}{\\partial b}\r\n",
        "\\end{align}\r\n",
        "$$\r\n",
        "\r\n",
        "The parameter $\\alpha$ is typically referred to as the \"learning rate,\" and will be defined later in the code. \r\n",
        "\r\n",
        "When doing training, we pass a mini-batch of data as input, perform a sequence of calculations to obtain the loss, then propagate back through the network to calculate the derivatives used in the gradient descent formulas above. Once we have the derivatives, we can update the values of the network's parameters $W$ and $b$ according to the formulas. This sequence of steps is the backpropagation algorithm. By performing these calculations several times, our parameters get updated repeatedly, getting more and more accurate each time. \r\n",
        "\r\n",
        "In Keras, when we called the function [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), the backpropagation algorithm was executed several times. Here, we'll start by understanding the code that reflects a single pass of the backpropagation algorithm:\r\n",
        "\r\n",
        "- A forward pass through the model to compute the predicted value, `y_prime = model(X, training=True)`\r\n",
        "- A calculation of the loss using a loss function, `loss = loss_fn(y, y_prime)`\r\n",
        "- A backward pass from the loss function through the model to calculate derivatives, `grads = tape.gradient(loss, model.trainable_variables)`\r\n",
        "- A gradient descent step to update $W$ and $b$ using the derivatives calculated in the backward pass, `optimizer.apply_gradients(zip(grads, model.trainable_variables))`\r\n",
        "\r\n",
        "Here's the complete code:\r\n",
        "\r\n",
        "\r\n",
        "参数 $\\alpha$ 通常称为“学习率”，稍后将在代码中定义。\r\n",
        "\r\n",
        "在进行训练时，我们将一小批数据作为输入，执行一系列计算以获得损失，然后通过网络传播回去以计算上述梯度下降公式中使用的导数。 一旦我们有了导数，我们就可以根据公式更新网络参数 $W$ 和 $b$ 的值。 这一系列步骤就是反向传播算法。 通过多次执行这些计算，我们的参数得到反复更新，每次都变得越来越准确。\r\n",
        "\r\n",
        "在 Keras 中，当我们调用函数 [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) 时，反向传播算法被执行了几次。 在这里，我们将从了解反映单次反向传播算法的代码开始：\r\n",
        "\r\n",
        "- 前向通过模型计算预测值，`y_prime = model(X, training=True)`\r\n",
        "- 使用损失函数计算损失，`loss = loss_fn(y, y_prime)`\r\n",
        "- 从损失函数向后通过模型计算导数，`grads = tape.gradient(loss, model.trainable_variables)`\r\n",
        "- 使用在反向传递中计算的导数更新 $W$ 和 $b$ 的梯度下降步骤，`optimizer.apply_gradients(zip(grads, model.trainable_variables))`\r\n",
        "\r\n",
        "这是完整的代码："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_one_batch(X, y, model, loss_fn, optimizer) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_prime = model(X, training=True)\n",
        "    loss = loss_fn(y, y_prime)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return (y_prime, loss)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the code above ensures that the forward calculations are within the `GradientTape`'s scope, just as we saw in the previous notebook. This makes it possible for us to ask the tape for the gradients. \n",
        "\n",
        "The code above works for a single mini-batch, which is typically much smaller than the full set of data (in this sample we use a mini-batch of size 64, out of 60,000 training data items). But we want to execute the backpropagation algorithm for the full set of data. We can do so by iterating through the `Dataset` we created earlier, which, as we saw in module 1, returns a mini-batch per iteration. There are two critical lines in the code below: the `for` loop and the call to the `fit_one_batch` function. The rest of the code just prints the accuracy and loss as the model is being trained. \n",
        "\n",
        "请注意，上面的代码确保前向计算在`GradientTape` 的范围内，就像我们在之前的笔记本中看到的那样。 这使我们可以向磁带询问梯度。\n",
        "\n",
        "上面的代码适用于单个小批量，它通常比完整的数据集小得多（在这个示例中，我们使用大小为 64 的小批量，来自 60,000 个训练数据项）。 但是我们要对整组数据执行反向传播算法。 我们可以通过迭代我们之前创建的`Dataset`来做到这一点，正如我们在模块 1 中看到的那样，每次迭代返回一个小批量。 下面的代码中有两个关键行：`for`循环和对`fit_one_batch`函数的调用。 其余代码仅在训练模型时打印准确性和损失。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(dataset: tf.data.Dataset, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.optimizers.Optimizer) -> None:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "  print_every = 100\n",
        "\n",
        "  for batch_index, (X, y) in enumerate(dataset):\n",
        "    (y_prime, loss) = fit_one_batch(X, y, model, loss_fn, optimizer)\n",
        "\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1) == y).numpy().sum()\n",
        "\n",
        "    batch_loss = loss.numpy()\n",
        "    loss_sum += batch_loss\n",
        "    current_item_count += len(X)\n",
        "\n",
        "    if ((batch_index + 1) % print_every == 0) or ((batch_index + 1) == batch_count):\n",
        "      batch_accuracy = correct_item_count / current_item_count * 100\n",
        "      print(f'[Batch {batch_index + 1:>3d} - {current_item_count:>5d} items] accuracy: {batch_accuracy:>0.1f}%, loss: {batch_loss:>7f}')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A complete iteration over all mini-batches in the dataset is called an \"epoch.\" In this sample, we restrict the code to just five epochs for quick execution, but in a real project you would want to set it to a much higher number (to achieve better predictions). The code below also shows the creation of the loss function and optimizer, which we discussed in module 1.\r\n",
        "\r\n",
        "对数据集中所有小批量的完整迭代称为`epoch`。 在此示例中，我们将代码限制为仅五个时期以便快速执行，但在实际项目中，您可能希望将其设置为更高的数字（以实现更好的预测）。 下面的代码还显示了我们在模块 1 中讨论的损失函数和优化器的创建。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "(train_dataset, test_dataset) = get_data(batch_size)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "print('\\nFitting:')\n",
        "for epoch in range(epochs):\n",
        "  print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
        "  fit(train_dataset, model, loss_fn, optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nFitting:\n\nEpoch 1\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 62.9%, loss: 0.705272\n[Batch 200 - 12800 items] accuracy: 68.9%, loss: 0.743024\n[Batch 300 - 19200 items] accuracy: 71.5%, loss: 0.531616\n[Batch 400 - 25600 items] accuracy: 73.6%, loss: 0.617099\n[Batch 500 - 32000 items] accuracy: 74.7%, loss: 0.561903\n[Batch 600 - 38368 items] accuracy: 75.7%, loss: 0.925934\n[Batch 700 - 44768 items] accuracy: 76.6%, loss: 0.632339\n[Batch 800 - 51168 items] accuracy: 77.3%, loss: 0.370619\n[Batch 900 - 57568 items] accuracy: 77.9%, loss: 0.568127\n[Batch 938 - 60000 items] accuracy: 78.1%, loss: 0.627154\n\nEpoch 2\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 83.6%, loss: 0.487360\n[Batch 200 - 12800 items] accuracy: 83.4%, loss: 0.428791\n[Batch 300 - 19200 items] accuracy: 83.3%, loss: 0.486824\n[Batch 400 - 25600 items] accuracy: 83.4%, loss: 0.391912\n[Batch 500 - 32000 items] accuracy: 83.5%, loss: 0.424991\n[Batch 600 - 38368 items] accuracy: 83.6%, loss: 0.515074\n[Batch 700 - 44768 items] accuracy: 83.6%, loss: 0.485272\n[Batch 800 - 51168 items] accuracy: 83.6%, loss: 0.345185\n[Batch 900 - 57568 items] accuracy: 83.7%, loss: 0.466114\n[Batch 938 - 60000 items] accuracy: 83.8%, loss: 0.271606\n\nEpoch 3\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 85.3%, loss: 0.462108\n[Batch 200 - 12800 items] accuracy: 85.0%, loss: 0.370773\n[Batch 300 - 19200 items] accuracy: 85.2%, loss: 0.467257\n[Batch 400 - 25600 items] accuracy: 85.1%, loss: 0.564032\n[Batch 500 - 32000 items] accuracy: 85.1%, loss: 0.419473\n[Batch 600 - 38400 items] accuracy: 85.1%, loss: 0.444543\n[Batch 700 - 44768 items] accuracy: 85.1%, loss: 0.477494\n[Batch 800 - 51168 items] accuracy: 85.2%, loss: 0.428996\n[Batch 900 - 57568 items] accuracy: 85.2%, loss: 0.377139\n[Batch 938 - 60000 items] accuracy: 85.2%, loss: 0.331598\n\nEpoch 4\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 85.8%, loss: 0.400852\n[Batch 200 - 12800 items] accuracy: 85.5%, loss: 0.399611\n[Batch 300 - 19200 items] accuracy: 85.6%, loss: 0.363247\n[Batch 400 - 25600 items] accuracy: 85.7%, loss: 0.337134\n[Batch 500 - 32000 items] accuracy: 85.8%, loss: 0.366630\n[Batch 600 - 38368 items] accuracy: 85.7%, loss: 0.488763\n[Batch 700 - 44768 items] accuracy: 85.7%, loss: 0.640464\n[Batch 800 - 51168 items] accuracy: 85.8%, loss: 0.422981\n[Batch 900 - 57568 items] accuracy: 85.9%, loss: 0.242606\n[Batch 938 - 60000 items] accuracy: 85.9%, loss: 0.532112\n\nEpoch 5\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 86.5%, loss: 0.347542\n[Batch 200 - 12800 items] accuracy: 86.4%, loss: 0.427709\n[Batch 300 - 19200 items] accuracy: 86.5%, loss: 0.335535\n[Batch 400 - 25600 items] accuracy: 86.5%, loss: 0.185783\n[Batch 500 - 32000 items] accuracy: 86.6%, loss: 0.307705\n[Batch 600 - 38400 items] accuracy: 86.6%, loss: 0.274786\n[Batch 700 - 44800 items] accuracy: 86.6%, loss: 0.315632\n[Batch 800 - 51168 items] accuracy: 86.6%, loss: 0.267578\n[Batch 900 - 57568 items] accuracy: 86.6%, loss: 0.252907\n[Batch 938 - 60000 items] accuracy: 86.6%, loss: 0.485074\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "azureml_py38"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}