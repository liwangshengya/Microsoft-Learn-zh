{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "In our previous example, we operated on high-dimensional bag-of-words vectors with length `vocab_size`, and we explicitly converted low-dimensional positional representation vectors into sparse one-hot representation. This one-hot representation is not memory-efficient. In addition, each word is treated independently from each other, so one-hot encoded vectors don't express semantic similarities between words.\n",
        "\n",
        "In this unit, we will continue exploring the **News AG** dataset. To begin, let's load the data and get some definitions from the previous unit.\n",
        "\n",
        "在我们之前的示例中，我们对长度为“vocab_size”的高维词袋向量进行操作，并将低维位置表示向量显式转换为稀疏的单热表示。 这种单热表示的内存效率不高。 此外，每个单词都是相互独立处理的，因此单热编码向量不表达单词之间的语义相似性。\n",
        "\n",
        "在本单元中，我们将继续探索 **News AG** 数据集。 首先，让我们加载数据并从上一个单元获取一些定义。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
        "# we will set tensorflow option to grow GPU memory allocation when required.\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "if len(physical_devices)>0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What's an embedding?\n",
        "\n",
        "The idea of **embedding** is to represent words using lower-dimensional dense vectors that reflect the semantic meaning of the word. We will later discuss how to build meaningful word embeddings, but for now let's just think of embeddings as a way to reduce the dimensionality of a word vector. \n",
        "\n",
        "So, an embedding layer takes a word as input, and produces an output vector of specified `embedding_size`. In a sense, it is very similar to a `Dense` layer, but instead of taking a one-hot encoded vector as input, it's able to take a word number.\n",
        "\n",
        "By using an embedding layer as the first layer in our network, we can switch from bag-or-words to an **embedding bag** model, where we first convert each word in our text into the corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `average` or `max`.  \n",
        "\n",
        "**embedding** 的思想是使用反映单词语义的低维稠密向量来表示单词。 稍后我们将讨论如何构建有意义的词嵌入，但现在让我们将嵌入视为降低词向量维数的一种方式。\n",
        "\n",
        "因此，嵌入层将单词作为输入，并生成指定`embedding_size`的输出向量。 从某种意义上说，它与`Dense`层非常相似，但它不是将单热编码向量作为输入，而是能够采用单词编号。\n",
        "\n",
        "通过使用嵌入层作为我们网络中的第一层，我们可以从 bag-or-words 切换到 **embedding bag** 模型，我们首先将文本中的每个单词转换为相应的嵌入，然后计算一些 所有这些嵌入的聚合函数，例如 `sum`、`average` 或 `max`。\n",
        "\n",
        "![Image showing an embedding classifier for five sequence words.](./images/embedding-classifier-example.png)\n",
        "\n",
        "Our classifier neural network consists of the following layers:\n",
        "\n",
        "* `TextVectorization` layer, which takes a string as input, and produces a tensor of token numbers. We will specify some reasonable vocabulary size `vocab_size`, and ignore less-frequently used words. The input shape will be 1, and the output shape will be $n$, since we'll get $n$ tokens as a result, each of them containing numbers from 0 to `vocab_size`.\n",
        "* `Embedding` layer, which takes $n$ numbers, and reduces each number to a dense vector of a given length (100 in our example). Thus, the input tensor of shape $n$ will be transformed into an $n\\times 100$ tensor. \n",
        "* Aggregation layer, which takes the average of this tensor along the first axis, i.e. it will compute the average of all $n$ input tensors corresponding to different words. To implement this layer, we will use a `Lambda` layer, and pass into it the function to compute the average. The output will have shape of 100, and it will be the numeric representation of the whole input sequence.\n",
        "* Final `Dense` linear classifier.\n",
        "\n",
        "我们的分类器神经网络由以下层组成：\n",
        "\n",
        "* `TextVectorization` 层，它以字符串作为输入，并生成标记数字的张量。 我们将指定一些合理的词汇量 `vocab_size`，并忽略不常用的词。 输入形状将为 1，输出形状将为 $n$，因为我们将得到 $n$ 个标记作为结果，每个标记都包含从 0 到 `vocab_size` 的数字。\n",
        "* `Embedding` 层，它采用 $n$ 个数字，并将每个数字缩减为给定长度的密集向量（在我们的示例中为 100）。 因此，形状为 $n$ 的输入张量将被转换为 $n\\times 100$ 张量。\n",
        "* 聚合层，沿第一个轴取该张量的平均值，即它将计算与不同单词对应的所有 $n$ 个输入张量的平均值。 为了实现这一层，我们将使用一个`Lambda`层，并将计算平均值的函数传递给它。 输出的形状为 100，它将是整个输入序列的数字表示。\n",
        "* 最终的`Dense`线性分类器。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "batch_size = 128\n",
        "\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,    \n",
        "    keras.layers.Embedding(vocab_size,100),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization_1 (TextVe (None, None)              0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, None, 100)         3000000   \n_________________________________________________________________\nlambda_1 (Lambda)            (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4)                 404       \n=================================================================\nTotal params: 3,000,404\nTrainable params: 3,000,404\nNon-trainable params: 0\n_________________________________________________________________\n"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `summary` printout, in the **output shape** column, the first tensor dimension `None` corresponds to the minibatch size, and the second corresponds to the length of the token sequence. All token sequences in the minibatch have different lengths. We'll discuss how to deal with it in the next section.\n",
        "\n",
        "Now let's train the network:\n",
        "\n",
        "在 `summary` 打印输出中，在 **output shape** 列中，第一个张量维度 `None` 对应于小批量大小，第二个对应于标记序列的长度。 minibatch 中的所有 token 序列都有不同的长度。 我们将在下一节讨论如何处理它。\n",
        "\n",
        "现在让我们训练网络："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "print(\"Training vectorizer\")\n",
        "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training vectorizer\n938/938 [==============================] - 12s 13ms/step - loss: 0.7953 - acc: 0.8113 - val_loss: 0.4496 - val_acc: 0.8657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0f647e5490>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note** that we are building vectorizer based on a subset of the data. This is done in order to speed up the process, and it might result in a situation when not all tokens from our text is present in the vocabulary. In this case, those tokens would be ignored, which may result in slightly lower accuracy. However, in real life a subset of text often gives a good vocabulary estimation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **注意**我们正在基于数据的子集构建向量化器。 这样做是为了加快这个过程，并且可能会导致并非我们文本中的所有标记都出现在词汇表中的情况。 在这种情况下，这些标记将被忽略，这可能会导致准确性略低。 然而，在现实生活中，文本的子集通常可以提供很好的词汇量估计。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with variable sequence sizes\n",
        "\n",
        "Let's understand how training happens in minibatches. In the example above, the input tensor has dimension 1, and we use 128-long minibatches, so that actual size of the tensor is $128 \\times 1$. However, the number of tokens in each sentence is different. If we apply the `TextVectorization` layer to a single input, the number of tokens returned is different, depending on how the text is tokenized:\n",
        "\n",
        "让我们了解训练是如何在小批量中进行的。 在上面的示例中，输入张量的维度为 1，我们使用 128 长的小批量，因此张量的实际大小为 $128 \\times 1$。 但是，每个句子中的 token 数量是不同的。 如果我们将 `TextVectorization` 层应用于单个输入，则返回的标记数量会有所不同，具体取决于文本的标记化方式："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer('Hello, world!'))\n",
        "print(vectorizer('I am glad to meet you!'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\ntf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when we apply the vectorizer to several sequences, it has to produce a tensor of rectangular shape, so it fills unused elements with the PAD token (which in our case is zero):\r\n",
        "\r\n",
        "然而，当我们将向量化器应用于多个序列时，它必须产生一个矩形张量，因此它用 PAD 标记（在我们的例子中为零）填充未使用的元素："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['Hello, world!','I am glad to meet you!'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\narray([[   1,   45,    0,    0,    0,    0],\n       [ 112, 1271,    1,    3, 1747,  158]])>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the embeddings:\r\n",
        "\r\n",
        "在这里我们可以看到嵌入："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "array([[[-0.02485236, -0.00416857, -0.06599288, ..., -0.02404598,\n          0.03529833, -0.02100844],\n        [ 0.22493948,  0.01383338,  0.12420551, ...,  0.19531338,\n          0.13524376,  0.04216914],\n        [ 0.04510409,  0.00708018, -0.0310419 , ..., -0.0188726 ,\n         -0.0179676 , -0.04813331],\n        [ 0.04510409,  0.00708018, -0.0310419 , ..., -0.0188726 ,\n         -0.0179676 , -0.04813331],\n        [ 0.04510409,  0.00708018, -0.0310419 , ..., -0.0188726 ,\n         -0.0179676 , -0.04813331],\n        [ 0.04510409,  0.00708018, -0.0310419 , ..., -0.0188726 ,\n         -0.0179676 , -0.04813331]],\n\n       [[-0.00226152, -0.0972852 , -0.00063103, ...,  0.00504377,\n          0.22460397,  0.1497297 ],\n        [-0.15621698, -0.13758421, -0.02889572, ..., -0.02577994,\n          0.03472563,  0.08767739],\n        [-0.02485236, -0.00416857, -0.06599288, ..., -0.02404598,\n          0.03529833, -0.02100844],\n        [-0.06490357, -0.08200071, -0.06175491, ..., -0.02477042,\n         -0.06802022, -0.01040947],\n        [ 0.03279151,  0.12563369,  0.06062867, ..., -0.04349922,\n         -0.12154414, -0.12533969],\n        [-0.14435016, -0.304014  , -0.00378676, ...,  0.05609043,\n          0.20370889,  0.28518862]]], dtype=float32)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: To minimize the amount of padding, in some cases it makes sense to sort all sequences in the dataset in the order of increasing length (or, more precisely, number of tokens). This will ensure that each minibatch contains sequences of similar length."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **注意**：为了最小化填充量，在某些情况下，按长度（或更准确地说，标记数）递增的顺序对数据集中的所有序列进行排序是有意义的。 这将确保每个小批量包含相似长度的序列。"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Semantic embeddings: Word2Vec\n",
        "\n",
        "In our previous example, the embedding layer learned to map words to vector representations, however, these representations did not have semantic meaning. It would be nice to learn a vector representation such that similar words or synonyms correspond to vectors that are close to each other in terms of some vector distance (for example euclidian distance).\n",
        "\n",
        "To do that, we need to pretrain our embedding model on a large collection of text using a technique such as [Word2Vec](https://en.wikipedia.org/wiki/Word2vec). It's based on two main architectures that are used to produce a distributed representation of words:\n",
        "\n",
        " - **Continuous bag-of-words** (CBoW), where we train the model to predict a word from the surrounding context. Given the ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, the goal of the model is to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.\n",
        " - **Continuous skip-gram** is the opposite of CBoW. The model uses the input word ($W_0$) to predict the surrounding window of context words.\n",
        "\n",
        "CBoW is faster, and while skip-gram is slower, it does a better job of representing infrequent words.\n",
        "\n",
        "在我们之前的例子中，嵌入层学会了将单词映射到向量表示，然而，这些表示没有语义意义。 最好学习一种向量表示，这样相似的单词或同义词对应于在某种向量距离（例如欧几里得距离）方面彼此接近的向量。\n",
        "\n",
        "为此，我们需要使用 [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) 等技术在大量文本上预训练我们的嵌入模型。 它基于两个主要的架构，用于生成单词的分布式表示：\n",
        "\n",
        "  - **连续词袋**（CBoW），我们训练模型从周围的上下文中预测一个词。 给定 ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$，模型的目标是从 $(W_{-2},W_{-1} ,W_1,W_2)$。\n",
        "  - **Continuous skip-gram** 与 CBoW 相反。 该模型使用输入词（$W_0$）来预测上下文词的周围窗口。\n",
        "\n",
        "CBoW 更快，虽然 skip-gram 更慢，但它在表示不常用词方面做得更好。\n",
        "\n",
        "![Image showing both CBoW and Skip-Gram algorithms to convert words to vectors.](./images/example-algorithms-for-converting-words-to-vectors.png)\n",
        "\n",
        "To experiment with the Word2Vec embedding pretrained on Google News dataset, we can use the **gensim** library. Below we find the words most similar to 'neural'.\n",
        "\n",
        "> **Note:** When you first create word vectors, downloading them can take some time!\n",
        "\n",
        "为了试验在 Google 新闻数据集上预训练的 Word2Vec 嵌入，我们可以使用 **gensim** 库。 下面我们找到与“神经”最相似的词。\n",
        "\n",
        "> **注意：** 当您第一次创建词向量时，下载它们可能需要一些时间！"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "neuronal -> 0.7804799675941467\nneurons -> 0.7326500415802002\nneural_circuits -> 0.7252851724624634\nneuron -> 0.7174385190010071\ncortical -> 0.6941086649894714\nbrain_circuitry -> 0.6923246383666992\nsynaptic -> 0.6699118614196777\nneural_circuitry -> 0.6638563275337219\nneurochemical -> 0.6555314064025879\nneuronal_activity -> 0.6531826257705688\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also extract the vector embedding from the word, to be used in training the classification model. The embedding has 300 components, but here we only show the first 20 components of the vector for clarity:\r\n",
        "\r\n",
        "我们还可以从单词中提取向量嵌入，用于训练分类模型。 嵌入有 300 个分量，但为了清楚起见，这里我们只显示向量的前 20 个分量："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['play'][:20]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n      dtype=float32)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The great thing about semantic embeddings is that you can manipulate the vector encoding based on semantics. For example, we can ask to find a word whose vector representation is as close as possible to the words *king* and *woman*, and as far as possible from the word *man*:\r\n",
        "\r\n",
        "语义嵌入的伟大之处在于您可以根据语义操纵矢量编码。 例如，我们可以要求找到一个词，其向量表示尽可能接近词 *king* 和 *woman*，并尽可能远离词 *man*："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "('queen', 0.7118192911148071)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example above uses some internal GenSym magic, but the underlying logic is actually quite simple. An interesting thing about embeddings is that you can perform normal vector operations on embedding vectors, and that would reflect operations on word **meanings**. The example above can be expressed in terms of vector operations: we calculate the vector corresponding to **KING-MAN+WOMAN** (operations `+` and `-` are performed on vector representations of corresponding words), and then find the closest word in the dictionary to that vector:\r\n",
        "\r\n",
        "上面的示例使用了一些内部 GenSym 魔法，但底层逻辑实际上非常简单。 关于嵌入的一个有趣的事情是，您可以对嵌入向量执行法向量运算，这将反映对词**含义**的操作。 上面的例子可以用向量运算来表示：我们计算**KING-MAN+WOMAN**对应的向量（对对应词的向量表示进行`+`和`-`运算），然后找到 字典中最接近该向量的词：\r\n",
        "按Esc 停止编辑\r\n",
        "上面的例子使用了一些 inter"
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
        "# find the index of the closest embedding vector \n",
        "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
        "min_idx = np.argmin(d)\n",
        "# find the corresponding word\n",
        "w2v.index2word[min_idx]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'queen'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **NOTE**: We had to add a small coefficients to *man* and *woman* vectors - try removing them to see what happens.\n",
        "\n",
        "To find the closest vector, we use TensorFlow machinery to compute a vector of distances between our vector and all vectors in the vocabulary, and then find the index of minimal word using `argmin`.\n",
        "\n",
        "> **注意**：我们必须向 *man* 和 *woman* 向量添加一个小系数 - 尝试删除它们以查看会发生什么。\n",
        "\n",
        "为了找到最接近的向量，我们使用 TensorFlow 机制来计算我们的向量与词汇表中所有向量之间的距离向量，然后使用 argmin 找到最小词的索引。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Word2Vec seems like a great way to express word semantics, it has many disadvantages, including the following:\n",
        "\n",
        "* Both CBoW and skip-gram models are **predictive embeddings**, and they only take local context into account. Word2Vec does not take advantage of global context.\n",
        "* Word2Vec does not take into account word **morphology**, i.e. the fact that the meaning of the word can depend on different parts of the word, such as the root.  \n",
        "\n",
        "**FastText** tries to overcome the second limitation, and builds on Word2Vec by learning vector representations for each word and the character n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pretraining, it enables word embeddings to encode sub-word information.\n",
        "\n",
        "Another method, **GloVe**, uses a different approach to word embeddings, based on the factorization of the word-context matrix. First, it builds a large matrix that counts the number of word occurences in different contexts, and then it tries to represent this matrix in lower dimensions in a way that minimizes reconstruction loss.\n",
        "\n",
        "The gensim library supports those word embeddings, and you can experiment with them by changing the model loading code above.\n",
        "\n",
        "虽然 Word2Vec 似乎是表达单词语义的好方法，但它有很多缺点，包括以下内容：\n",
        "\n",
        "* CBoW 和 skip-gram 模型都是**预测嵌入**，它们只考虑局部上下文。 Word2Vec 不利用全局上下文。\n",
        "* Word2Vec 没有考虑单词**形态**，即单词的含义可能取决于单词的不同部分，例如词根。\n",
        "\n",
        "**FastText** 试图克服第二个限制，并通过学习每个单词的向量表示和在每个单词中找到的字符 n-gram 来构建 Word2Vec。 然后在每个训练步骤将表示的值平均到一个向量中。 虽然这为预训练增加了大量额外的计算，但它使词嵌入能够对子词信息进行编码。\n",
        "\n",
        "另一种方法，**GloVe**，使用不同的词嵌入方法，基于词上下文矩阵的分解。 首先，它构建了一个大矩阵来计算单词在不同上下文中出现的次数，然后它尝试以最小化重建损失的方式在较低维度中表示该矩阵。\n",
        "\n",
        "gensim 库支持这些词嵌入，您可以通过更改上面的模型加载代码来试验它们。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using pretrained embeddings in Keras\n",
        "\n",
        "We can modify the example above to prepopulate the matrix in our embedding layer with semantic embeddings, such as Word2Vec. The vocabularies of the pretrained embedding and the text corpus will likely not match, so we need to choose one. Here we explore the two possible options: using the tokenizer vocabulary, and using the vocabulary from Word2Vec embeddings.\n",
        "\n",
        "我们可以修改上面的示例，以使用语义嵌入（例如 Word2Vec）在我们的嵌入层中预填充矩阵。 预训练嵌入的词汇和文本语料库中的词汇很可能不匹配，所以我们需要选择一个。 在这里，我们探讨了两种可能的选择：使用分词器词汇表，以及使用来自 Word2Vec 嵌入的词汇表。\n",
        "\n",
        "### Using tokenizer vocabulary\n",
        "\n",
        "When using the tokenizer vocabulary, some of the words from the vocabulary will have corresponding Word2Vec embeddings, and some will be missing. Given that our vocabulary size is `vocab_size`, and the Word2Vec embedding vector length is `embed_size`, the embedding layer will be repesented by a weight matrix of shape `vocab_size`$\\times$`embed_size`. We will populate this matrix by going through the vocabulary:\n",
        "\n",
        "使用 tokenizer 词汇表时，词汇表中的某些词会有相应的 Word2Vec 嵌入，而某些词会缺失。 鉴于我们的词汇量大小为`vocab_size`，Word2Vec 嵌入向量长度为`embed_size`，嵌入层将由形状为`vocab_size`$\\times$`embed_size`的权重矩阵表示。 我们将通过词汇表来填充这个矩阵："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "W = np.zeros((vocab_size,embed_size))\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab):\n",
        "    try:\n",
        "        W[i] = w2v.get_vector(w)\n",
        "        found+=1\n",
        "    except:\n",
        "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Embedding size: 300\nPopulating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For words that are not present in the Word2Vec vocabulary, we can either leave them as zeroes, or generate a random vector.\n",
        "\n",
        "Now we can define an embedding layer with pretrained weights:\n",
        "\n",
        "对于 Word2Vec 词汇表中不存在的词，我们可以将它们保留为零，或者生成一个随机向量。\n",
        "\n",
        "现在我们可以定义一个带有预训练权重的嵌入层："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer, emb,\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train our model. \r\n",
        "\r\n",
        "现在让我们训练我们的模型。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
        "          validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "938/938 [==============================] - 6s 7ms/step - loss: 1.1098 - acc: 0.7849 - val_loss: 0.9145 - val_acc: 0.8159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0da2028b90>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: Notice that we set `trainable=False` when creating the `Embedding`, which means that we're not retraining the Embedding layer. This may cause accuracy to be slightly lower, but it speeds up the training.\n",
        "\n",
        "zh\n",
        "\n",
        "> **注意**：请注意，我们在创建 `Embedding` 时设置了 `trainable=False`，这意味着我们没有重新训练 Embedding 层。 这可能会导致准确性略低，但会加快训练速度。\n",
        "\n",
        "### Using embedding vocabulary\n",
        "\n",
        "One issue with the previous approach is that the vocabularies used in the TextVectorization and Embedding are different. To overcome this problem, we can use one of the following solutions:\n",
        "* Re-train the Word2Vec model on our vocabulary.\n",
        "* Load our dataset with the vocabulary from the pretrained Word2Vec model. Vocabularies used to load the dataset can be specified during loading.\n",
        "\n",
        "The latter approach seems easier, so let's implement it. First of all, we will create a `TextVectorization` layer with the specified vocabulary, taken from the Word2Vec embeddings:\n",
        "\n",
        "先前方法的一个问题是 TextVectorization 和 Embedding 中使用的词汇表不同。 为了克服这个问题，我们可以使用以下解决方案之一：\n",
        "* 在我们的词汇表上重新训练 Word2Vec 模型。\n",
        "* 使用预训练的 Word2Vec 模型中的词汇加载我们的数据集。 可以在加载期间指定用于加载数据集的词汇表。\n",
        "\n",
        "后一种方法似乎更容易，所以让我们来实现它。 首先，我们将使用取自 Word2Vec 嵌入的指定词汇创建一个 `TextVectorization` 层："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(w2v.vocab.keys())\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
        "vectorizer.set_vocabulary(vocab)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gensim word embeddings library contains a convenient function, `get_keras_embeddings`, which will automatically create the corresponding Keras embeddings layer for you.\r\n",
        "\r\n",
        "gensim 词嵌入库包含一个方便的函数 `get_keras_embeddings`，它会自动为您创建相应的 Keras 嵌入层。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    vectorizer, \n",
        "    w2v.get_keras_embedding(train_embeddings=False),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n938/938 [==============================] - 7s 7ms/step - loss: 1.3381 - acc: 0.4961 - val_loss: 1.2996 - val_acc: 0.5682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/5\n938/938 [==============================] - 7s 7ms/step - loss: 1.2591 - acc: 0.5714 - val_loss: 1.2340 - val_acc: 0.5839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/5\n938/938 [==============================] - 7s 7ms/step - loss: 1.1983 - acc: 0.5883 - val_loss: 1.1827 - val_acc: 0.5951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/5\n938/938 [==============================] - 7s 7ms/step - loss: 1.1505 - acc: 0.6001 - val_loss: 1.1417 - val_acc: 0.6021\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/5\n938/938 [==============================] - 7s 7ms/step - loss: 1.1122 - acc: 0.6093 - val_loss: 1.1084 - val_acc: 0.6103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0da1d67110>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the reasons we're not seeing higher accuracy is because some words from our dataset are missing in the pretrained GloVe vocabulary, and thus they are essentially ignored. To overcome this, we can train our own embeddings based on our dataset. \n",
        "\n",
        "我们没有看到更高准确度的原因之一是因为我们数据集中的一些词在预训练的 GloVe 词汇表中缺失，因此它们基本上被忽略了。 为了克服这个问题，我们可以根据我们的数据集训练我们自己的嵌入。\n",
        "\n",
        "\n",
        "## Training your own embeddings\n",
        "\n",
        "In our examples, we have been using pretrained semantic embeddings, but it is interesting to see how those embeddings can be trained using either CBoW, or skip-gram architectures. This exercise goes beyond this module, but those interested might want to check out this [official TensorFlow tutorial on training Word2Vec model](https://www.tensorflow.org/tutorials/text/word2vec). Also, the **gensim** framework can be used to train the most commonly used embeddings in a few lines of code, as described [in the official documentation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model).\n",
        "\n",
        "在我们的示例中，我们一直在使用预训练的语义嵌入，但有趣的是看看如何使用 CBoW 或 skip-gram 架构来训练这些嵌入。 此练习超出了本模块的范围，但有兴趣的人可能想查看此 [关于训练 Word2Vec 模型的官方 TensorFlow 教程](https://www.tensorflow.org/tutorials/text/word2vec)。 此外，**gensim** 框架可用于在几行代码中训练最常用的嵌入，如[在官方文档](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual embeddings\n",
        "\n",
        "One key limitation of traditional pretrained embedding representations such as Word2Vec is the fact that, even though they can capture some meaning of a word, they can't differentiate between different meanings. This can cause problems in downstream models.\n",
        "\n",
        "For example the word 'play' has different meaning in these two different sentences:\n",
        "- I went to a **play** at the theater.\n",
        "- John wants to **play** with his friends.\n",
        "\n",
        "The pretrained embeddings we talked about represent both meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the **language model**, which is trained on a large corpus of text, and *knows* how words can be put together in different contexts. Discussing contextual embeddings is out of scope for this tutorial, but we will come back to them when talking about language models in the next unit.\n",
        "\n",
        "\n",
        "传统的预训练嵌入表示（例如 Word2Vec）的一个关键限制是，即使它们可以捕获单词的某些含义，也无法区分不同的含义。 这可能会导致下游模型出现问题。\n",
        "\n",
        "例如，“play”这个词在这两个不同的句子中有不同的含义：\n",
        "- 我去剧院看了一场**戏**。\n",
        "- 约翰想和他的朋友们**玩**。\n",
        "\n",
        "我们讨论的预训练嵌入在同一个嵌入中代表了“玩”这个词的两种含义。 为了克服这一限制，我们需要基于**语言模型**构建嵌入，该模型在大量文本语料库上进行训练，并且*知道*如何将单词放在不同的上下文中。 讨论上下文嵌入超出了本教程的范围，但我们将在下一个单元中讨论语言模型时回到它们。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "conda-env-azureml_py38-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}