在此学习模块中，我们介绍了自然语言处理的所有基础知识，例如文本表示形式、传统的循环网络模型以及近乎最先进的注意力模型。 我们重点介绍了文本分类任务，没有详细讨论其他重要任务，例如命名实体识别、机器翻译和问题解答。 要实现这些任务，请将相同的基本 RNN 原则用于不同的顶层体系结构。 若要更全面地了解 NLP 领域，还应尝试其中一些问题。

NLP 的另一个新兴领域是模型可视化和探测。 此方向也称为 [BERTology](https://arxiv.org/abs/2002.12327)。 如上一单元中所述，可视化注意矩阵可以告诉我们很多关于机器翻译工作原理的信息，以及模型在翻译单词时“查看”的位置。 还有其他功能强大的方法可用于了解 BERT 内部机制。

最新的文本生成模型（如 GPT-2/3）与 BERT 略有不同，从某种意义上说，只需通过为文本生成提供“初始序列”，即可对它们进行“编程”以解决不同任务。 这可能导致范例转变，在这种情况下，我们将专注于为预先训练的大型网络创建合适的问题，而不是进行迁移学习训练。 若要真正了解 NLP，可能需要探索一些最新的文本生成模型，例如 [GPT-2](https://github.com/openai/gpt-2) 或 [Microsoft 图灵 NLG](https://www.microsoft.com/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)。

至此，你已了解有关执行任何自然语言任务的入门基础知识！
