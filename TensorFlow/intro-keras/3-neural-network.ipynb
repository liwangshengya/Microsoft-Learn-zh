{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is to classify an input image into one of the 10 classes of clothing, so we will define our neural network to take as input a matrix of shape (28, 28) and output a vector of size 10, where the index of the largest value in the output corresponds to the integer label for the class of clothing in the image. For example, if we use an image of an ankle boot as input, we might get an output vector $y'$ like this:\n",
        "\n",
        "我们的目标是将输入图像分类为 10 类服装中的一类，因此我们将定义神经网络以将形状为 (28, 28) 的矩阵作为输入并输出大小为 10 的向量，其中索引 输出中的最大值对应于图像中服装类别的整数标签。 例如，如果我们使用踝靴的图像作为输入，我们可能会得到这样的输出向量 $y'$：\n",
        "\n",
        "$$\n",
        "y' =\n",
        "\\begin{bmatrix}\n",
        "  0.0000 \\\\\n",
        "  5.3003 \\\\\n",
        "  2.1616 \\\\\n",
        "  1.9145 \\\\\n",
        "  0.0000 \\\\\n",
        "  5.1698 \\\\\n",
        "  0.0000 \\\\\n",
        "  2.2152 \\\\\n",
        "  0.0000 \\\\\n",
        "  7.0417 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this particular example, the largest value appears at index 9 (counting from zero) &mdash; and as we showed in the previous module, index 9 corresponds to the \"Ankle Boot\" category. So this indicates that our neural network correctly classified the image of an ankle boot.\n",
        "\n",
        "Here's a visualization of the structure of the neural network we chose for this scenario:\n",
        "\n",
        "在此特定示例中，最大值出现在索引 9（从零开始计数）—— 正如我们在上一个模块中展示的那样，索引 9 对应于“踝靴”类别。 所以这表明我们的神经网络正确地分类了踝靴的图像。\n",
        "\n",
        "这是我们为这个场景选择的神经网络结构的可视化：\n",
        "\n",
        "![Diagram of Fashion MNIST classification neural network](./images/1-fashion-nn.png)\n",
        "\n",
        "Because each image has 28 &times; 28 = 784 pixels, we need 784 nodes in the input layer (one for each pixel value). We decided to add one hidden layer with 20 nodes and a ReLU (rectified linear unit) activation function. We want the output of our network to be a vector of size 10, therefore our output layer needs to have 10 nodes.\n",
        "\n",
        "Here's the Keras code that defines this neural network:\n",
        "\n",
        "因为每个图像有 28 × 28 = 784 个像素，我们在输入层中需要 784 个节点（每个像素值一个）。 我们决定添加一个包含 20 个节点的隐藏层和一个 ReLU（整流线性单元）激活函数。 我们希望网络的输出是大小为 10 的向量，因此我们的输出层需要有 10 个节点。\n",
        "\n",
        "下面是定义此神经网络的 Keras 代码：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.sequence = tf.keras.Sequential([\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Dense(20, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "  def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "    y_prime = self.sequence(x)\n",
        "    return y_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layer turns our input matrix of shape (28, 28) into a vector of size 784. The [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers are also known as \"fully connected\" or \"linear\" layers because they connect all nodes from the previous layer with each of their own nodes using a linear function. Notice that they specify \"ReLU\" as the activation &mdash; that's because we want the results of the linear mathematical operation to get passed as input to a \"Rectified Linear Unit\" function, which adds non-linearity to the calculations. \n",
        "\n",
        "It's important to have non-linear activation functions (like the ReLU function) between linear layers, because otherwise a sequence of linear layers would be mathematically equivalent to just one layer. These activation functions give our network more expressive power, allowing it to approximate non-linear relationships between data.\n",
        "\n",
        "The [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class combines all the other layers. Lastly, we define the `call` method, which supplies a tensor `x` as input to the `sequence` of layers and produces the `y_prime` vector as a result.\n",
        "\n",
        "We can print a description of our model using the `summary` method:\n",
        "\n",
        "[`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) 层将形状为 (28, 28) 的输入矩阵转换为大小为 784 的向量。[ `Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) 层也称为“完全连接”或“线性”层，因为它们连接上一层的所有节点 每个节点都使用线性函数。 请注意，他们将“ReLU”指定为激活—— 那是因为我们希望将线性数学运算的结果作为输入传递给“修正线性单元”函数，这会在计算中增加非线性。\n",
        "\n",
        "在线性层之间使用非线性激活函数（如 ReLU 函数）很重要，否则线性层序列在数学上等同于一层。 这些激活函数赋予我们的网络更强的表达能力，使其能够近似数据之间的非线性关系。\n",
        "\n",
        "[`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) 类结合了所有其他层。 最后，我们定义了 `call` 方法，它提供一个张量 `x` 作为层的 `sequence` 的输入，并产生 `y_prime` 向量作为结果。\n",
        "\n",
        "我们可以使用 `summary` 方法打印我们模型的描述："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"neural_network\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 10)                15910     \n",
            "=================================================================\n",
            "Total params: 15,910\n",
            "Trainable params: 15,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork()\n",
        "model.build((1, 28, 28))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all the code needed to define our neural network. Now that we have a neural network and some data, it's time to train the neural network using that data. \n",
        "\n",
        "这是定义我们的神经网络所需的所有代码。 现在我们有了神经网络和一些数据，是时候使用这些数据训练神经网络了。"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b1e50b9be59f7fbb2b520cd9d0f0de3401ffabf78f05d86508e8222ccb7b50f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
