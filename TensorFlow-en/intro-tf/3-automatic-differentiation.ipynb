{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of the training process requires calculating derivatives that involve tensors. So let's learn about TensorFlow's built-in [automatic differentiation](https://www.tensorflow.org/guide/autodiff) engine, using a very simple example. Let's consider the following two tensors:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    1 & 2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    3 & 4 \\\\\n",
        "    5 & 6\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now let's suppose that we want to multiply $U$ by $V$, and then sum all the values in the resulting tensor, such that the result is a scalar. In math notation, we might represent this as the following scalar function $f$:\n",
        "\n",
        "$$\n",
        "f(U, V) = \\mathrm{sum} (U \\, V) = \\sum_j \\sum_i u_i \\, v_{ij}\n",
        "$$\n",
        "\n",
        "Our goal is to calculate the derivative of $f$ with respect to each of its inputs: $\\frac{\\partial f}{\\partial U}$ and $\\frac{\\partial f}{\\partial V}$. We start by creating the two tensors $U$ and $V$. We then create a [tf.GradientTape](https://www.tensorflow.org/guide/autodiff#gradient_tapes), and tell TensorFlow to watch for mathematical operations involving $U$ and $V$, recording those operations onto our \"tape.\" The tape then enables us to calculate the derivatives of the function $f$ with respect to $U$ and $V$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.constant([[1., 2.]])\n",
        "V = tf.constant([[3., 4.], [5., 6.]])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(U)\n",
        "  tape.watch(V)\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[ 7. 11.]], shape=(1, 2), dtype=float32)\ntf.Tensor(\n[[1. 1.]\n [2. 2.]], shape=(2, 2), dtype=float32)\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow automatically watches tensors that are defined as `Variable` instances. So let's turn `U` and `V` into variables, and remove the `watch` calls:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Decimal points in tensor values ensure they are floats, which automatic differentiation requires.\n",
        "U = tf.Variable(tf.constant([[1., 2.]]))\n",
        "V = tf.Variable(tf.constant([[3., 4.], [5., 6.]]))\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  W = tf.matmul(U, V)\n",
        "  f = tf.math.reduce_sum(W)\n",
        "\n",
        "print(tape.gradient(f, U)) # df/dU\n",
        "print(tape.gradient(f, V)) # df/dV"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tf.Tensor([[ 7. 11.]], shape=(1, 2), dtype=float32)\ntf.Tensor(\n[[1. 1.]\n [2. 2.]], shape=(2, 2), dtype=float32)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you will see later, in deep learning, we will need to calculate the derivatives of the loss function with respect to the model parameters. Those parameters are variables because they change during training. Therefore, the fact that variables are automatically watched is handy in our scenario.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional explanation of the math\n",
        "\n",
        "Let's take a look at the math used to compute the derivatives. You only need to understand matrix multiplication and partial derivatives to follow along, but if the math isn't as interesting to you, feel free to skip to the next notebook.\n",
        "\n",
        "We'll start by thinking of $U$ and $V$ as generic 1 &times; 2 and 2 &times; 2 matrices:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  U =\n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_2\n",
        "  \\end{bmatrix}\n",
        "  &&\n",
        "  V =\n",
        "  \\begin{bmatrix}\n",
        "    v_{11} & v_{12} \\\\\n",
        "    v_{21} & v_{22}\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Then the scalar function $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  f(U, V)\n",
        "  &= \\mathrm{sum}(U \\, V) \\\\\n",
        "  &= \\mathrm{sum} \n",
        "    \\left( \n",
        "      \\begin{bmatrix}\n",
        "        u_1 & u_2\n",
        "      \\end{bmatrix}\n",
        "      \\begin{bmatrix}\n",
        "        v_{11} & v_{12} \\\\\n",
        "        v_{21} & v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= \\mathrm{sum}\n",
        "    \\left(\n",
        "      \\begin{bmatrix}\n",
        "        u_1 v_{11} + u_2 v_{21} & u_1 v_{12} + u_2 v_{22}\n",
        "      \\end{bmatrix}\n",
        "    \\right) \\\\\n",
        "  &= u_1 v_{11} + u_2 v_{21} + u_1 v_{12} + u_2 v_{22}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "We can now calculate the derivatives of $f$ with respect to each of its inputs:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial U} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial u_1} & \\frac{\\partial f}{\\partial u_2}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    v_{11} + v_{12} & v_{21} + v_{22}\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    7 & 11\n",
        "  \\end{bmatrix} \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial V} =\n",
        "  \\begin{bmatrix}\n",
        "    \\frac{\\partial f}{\\partial v_{11}} & \\frac{\\partial f}{\\partial v_{12}} \\\\\n",
        "    \\frac{\\partial f}{\\partial v_{21}} & \\frac{\\partial f}{\\partial v_{22}} \n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    u_1 & u_1 \\\\\n",
        "    u_2 & u_2\n",
        "  \\end{bmatrix} = \n",
        "  \\begin{bmatrix}\n",
        "    1 & 1 \\\\\n",
        "    2 & 2\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "As you can see, when we plug in the numerical values of $U$ and $V$, we get the same result as TensorFlow's automatic differentiation.\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "azureml_py38"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}