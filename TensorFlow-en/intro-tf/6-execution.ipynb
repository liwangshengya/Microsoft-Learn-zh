{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up until now, we've been running our code in \"[eager execution](https://www.tensorflow.org/guide/eager)\" mode, which is enabled by default. In this mode, the flow of code execution happens in the order we're accustomed to, and we can add breakpoints and inspect the values of our tensors and variables as usual.\n",
        "\n",
        "In contrast, when in \"[graph execution](https://www.tensorflow.org/guide/intro_to_graphs)\" mode, the code execution flows a bit differently: during the first pass through the code, a computation graph is created containing information about the operations and tensors in that code. Then in subsequent passes, the graph is used instead of the Python code. One consequence of this flow is that our code is not debuggable in the usual manner. We gain two major advantages though:\n",
        "- The graph can be deployed to environments that don't have Python, such as embedded devices. \n",
        "- The graph can take advantage of several performance optimizations, such as running parts of the code in parallel.\n",
        "\n",
        "In order to get the best of both worlds, we use eager execution mode during the development phase, and then switch to graph execution mode once we're done debugging the model. To switch from eager to graph execution, we can add the `@tf.function` decorator to the function containing our model operations.\n",
        "\n",
        "Let's look at the training code again, but this time with the `@tf.function` decorator applied to the `fit_one_batch` function, which is where we have all the model operations."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-tf/tintro.py\n",
        "from tintro import *"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def fit_one_batch(X: tf.Tensor, y: tf.Tensor, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.keras.optimizers.Optimizer) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_prime = model(X, training=True)\n",
        "    loss = loss_fn(y, y_prime)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return (y_prime, loss)\n",
        "\n",
        "\n",
        "def fit(dataset: tf.data.Dataset, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.optimizers.Optimizer) -> None:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "  print_every = 100\n",
        "\n",
        "  for batch_index, (X, y) in enumerate(dataset):\n",
        "    (y_prime, loss) = fit_one_batch(X, y, model, loss_fn, optimizer)\n",
        "\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1) == y).numpy().sum()\n",
        "\n",
        "    batch_loss = loss.numpy()\n",
        "    loss_sum += batch_loss\n",
        "    current_item_count += len(X)\n",
        "\n",
        "    if ((batch_index + 1) % print_every == 0) or ((batch_index + 1) == batch_count):\n",
        "      batch_accuracy = correct_item_count / current_item_count * 100\n",
        "      print(f'[Batch {batch_index + 1:>3d} - {current_item_count:>5d} items] accuracy: {batch_accuracy:>0.1f}%, loss: {batch_loss:>7f}')\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "(train_dataset, test_dataset) = get_data(batch_size)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "print('\\nFitting:')\n",
        "t_begin = time.time()\n",
        "for epoch in range(epochs):\n",
        "  print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
        "  fit(train_dataset, model, loss_fn, optimizer)\n",
        "t_elapsed = time.time() - t_begin\n",
        "print(f'\\nTime per epoch: {t_elapsed / epochs :>.3f} sec' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nFitting:\n\nEpoch 1\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 60.1%, loss: 0.897071\n[Batch 200 - 12800 items] accuracy: 67.3%, loss: 0.578438\n[Batch 300 - 19200 items] accuracy: 70.9%, loss: 0.557362\n[Batch 400 - 25600 items] accuracy: 73.0%, loss: 0.760332\n[Batch 500 - 32000 items] accuracy: 74.4%, loss: 0.827212\n[Batch 600 - 38400 items] accuracy: 75.4%, loss: 0.288966\n[Batch 700 - 44768 items] accuracy: 76.5%, loss: 0.672835\n[Batch 800 - 51168 items] accuracy: 77.1%, loss: 0.716132\n[Batch 900 - 57568 items] accuracy: 77.6%, loss: 0.504184\n[Batch 938 - 60000 items] accuracy: 77.8%, loss: 0.445008\n\nEpoch 2\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 82.2%, loss: 0.480275\n[Batch 200 - 12800 items] accuracy: 83.0%, loss: 0.510372\n[Batch 300 - 19200 items] accuracy: 83.2%, loss: 0.764944\n[Batch 400 - 25600 items] accuracy: 83.2%, loss: 0.617230\n[Batch 500 - 32000 items] accuracy: 83.2%, loss: 0.440339\n[Batch 600 - 38400 items] accuracy: 83.3%, loss: 0.419223\n[Batch 700 - 44768 items] accuracy: 83.4%, loss: 0.701745\n[Batch 800 - 51168 items] accuracy: 83.4%, loss: 0.281240\n[Batch 900 - 57568 items] accuracy: 83.5%, loss: 0.380189\n[Batch 938 - 60000 items] accuracy: 83.6%, loss: 0.388993\n\nEpoch 3\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 83.2%, loss: 0.451256\n[Batch 200 - 12800 items] accuracy: 84.1%, loss: 0.515353\n[Batch 300 - 19200 items] accuracy: 84.3%, loss: 0.284418\n[Batch 400 - 25600 items] accuracy: 84.3%, loss: 0.475007\n[Batch 500 - 32000 items] accuracy: 84.4%, loss: 0.406650\n[Batch 600 - 38400 items] accuracy: 84.4%, loss: 0.518639\n[Batch 700 - 44800 items] accuracy: 84.5%, loss: 0.325835\n[Batch 800 - 51200 items] accuracy: 84.6%, loss: 0.283380\n[Batch 900 - 57600 items] accuracy: 84.7%, loss: 0.236743\n[Batch 938 - 60000 items] accuracy: 84.7%, loss: 0.305189\n\nEpoch 4\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 85.5%, loss: 0.511134\n[Batch 200 - 12800 items] accuracy: 85.2%, loss: 0.289383\n[Batch 300 - 19200 items] accuracy: 85.2%, loss: 0.366939\n[Batch 400 - 25600 items] accuracy: 85.4%, loss: 0.430243\n[Batch 500 - 32000 items] accuracy: 85.5%, loss: 0.415379\n[Batch 600 - 38368 items] accuracy: 85.3%, loss: 0.520753\n[Batch 700 - 44768 items] accuracy: 85.4%, loss: 0.377427\n[Batch 800 - 51168 items] accuracy: 85.4%, loss: 0.348202\n[Batch 900 - 57568 items] accuracy: 85.4%, loss: 0.318747\n[Batch 938 - 60000 items] accuracy: 85.4%, loss: 0.427576\n\nEpoch 5\n-------------------------------\n[Batch 100 -  6400 items] accuracy: 86.5%, loss: 0.357378\n[Batch 200 - 12800 items] accuracy: 86.2%, loss: 0.272055\n[Batch 300 - 19200 items] accuracy: 85.9%, loss: 0.404534\n[Batch 400 - 25600 items] accuracy: 85.9%, loss: 0.534800\n[Batch 500 - 32000 items] accuracy: 85.8%, loss: 0.533515\n[Batch 600 - 38400 items] accuracy: 85.8%, loss: 0.627523\n[Batch 700 - 44800 items] accuracy: 85.7%, loss: 0.424447\n[Batch 800 - 51200 items] accuracy: 85.7%, loss: 0.306241\n[Batch 900 - 57568 items] accuracy: 85.8%, loss: 0.311271\n[Batch 938 - 60000 items] accuracy: 85.8%, loss: 0.382301\n\nTime per epoch: 3.536 sec\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we also add a timer, and print the time it takes to train. You can comment and uncomment the `@tf.function` decorator, and notice the difference between the elapsed times. On my machine, eager execution takes more than twice the amount of time to train, compared to graph execution.\n",
        "\n",
        "Now that we've trained our model, we're ready to test it, which we can do by running a single pass forward through the network. The function `evaluate_one_batch` contains the code that does this: we simply need to call the `model` to get a prediction, followed by the loss function `loss_fn` to get a score for how the predicted labels `y_prime` compare to the actual labels `y`. Notice that we don't add a `tf.GradientTape()` this time &mdash; that's because, since we don't do a backward pass during testing, we don't need to calculate derivatives for gradient descent. Notice also that we added a `@tf.function` decorator once we were done with development and debugging, to get a performance boost.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def evaluate_one_batch(X: tf.Tensor, y: tf.Tensor, model: tf.keras.Model, \n",
        "loss_fn: tf.keras.losses.Loss) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  y_prime = model(X, training=False)\n",
        "  loss = loss_fn(y, y_prime)\n",
        "\n",
        "  return (y_prime, loss)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `evaluate` function calls the `evaluate_one_batch` function for the entire dataset, once per mini-batch. The important code in the function below is just the `for` loop and the call to `evaluate_one_batch` within it &mdash; the rest is just boilerplate code to print progress during execution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataset: tf.data.Dataset, model: tf.keras.Model, \n",
        "loss_fn: tf.keras.losses.Loss) -> Tuple[float, float]:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "\n",
        "  for (X, y) in dataset:\n",
        "    (y_prime, loss) = evaluate_one_batch(X, y, model, loss_fn)\n",
        "\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1).numpy() == y.numpy()).sum()\n",
        "    loss_sum += loss.numpy()\n",
        "    current_item_count += len(X)\n",
        "\n",
        "  average_loss = loss_sum / batch_count\n",
        "  accuracy = correct_item_count / current_item_count\n",
        "  return (average_loss, accuracy)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, we print the test loss and accuracy, and save the learned model parameters."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nEvaluating:')\n",
        "(test_loss, test_accuracy) = evaluate(test_dataset, model, loss_fn)\n",
        "print(f'Test accuracy: {test_accuracy * 100:>0.1f}%, test loss: {test_loss:>8f}')\n",
        "\n",
        "model.save_weights('outputs/weights')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nEvaluating:\nTest accuracy: 84.9%, test loss: 0.424060\n"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss and accuracy should be similar to the values we obtained with the Keras code. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "name": "conda-env-azureml_py38-py",
      "language": "python",
      "display_name": "azureml_py38"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}